{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Combined Dynamic Alpha + Selective N-Tokens Steering: Hallucination Guardrail Evaluation\n",
        " \n",
        "**Summary:**\n",
        "This notebook evaluates a combined guardrail approach for Llama-3.1-8B that integrates both Dynamic Alpha (risk-proportional steering strength) and Selective N-Tokens Steering (applying the intervention only to the first N tokens). This method applies a dynamic, risk-scaled correction for high-risk prompts, but only during the initial generation steps, maximizing hallucination reduction while minimizing latency and preserving answer quality. This combined approach outperforms both individual ablations and is used for all further evaluations on other datasets.\n",
        "\n",
        "- **Dynamic Alpha:** Steering strength (alpha) is scaled based on prompt risk, providing stronger correction for riskier prompts.\n",
        "- **Selective N-Tokens:** Steering is applied only to the first 10 generated tokens, focusing intervention where it is most effective.\n",
        "\n",
        "**Key Results (TruthfulQA Benchmark):**\n",
        "- **Baseline Model:** Accuracy: 38.57%, Hallucination Rate: 61.43%, Avg Latency: 3.86s\n",
        "- **Combined Guarded Model:** Accuracy: 52.04%, Hallucination Rate: 47.96%, Avg Latency: 3.56s\n",
        "- **Relative Error Reduction:** 21.93%\n",
        "- **Latency Increase:** -7.78% (latency decreased)\n",
        "\n",
        "This combined guardrail achieves the best trade-off between hallucination reduction, accuracy, and latency, and is therefore used for all subsequent cross-domain evaluations."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### **Environment and Requirements Setup**\n",
        "Setup for local execution on Lambda Labs A100 40GB GPU with Llama-3.1-8B."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NFpTX3fPzY6W",
        "outputId": "571ad1d1-3c15-423e-95b6-97f15a632507"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Environment verification:\n",
            "Python version: 3.10.12 (main, May 27 2025, 17:12:29) [GCC 11.4.0]\n",
            "PyTorch version: 2.9.0+cu128\n",
            "CUDA available: True\n",
            "CUDA version: 12.8\n",
            "GPU count: 1\n",
            "  GPU 0: NVIDIA A100-SXM4-40GB\n"
          ]
        }
      ],
      "source": [
        "\n",
        "import os\n",
        "import sys\n",
        "# Verify environment\n",
        "import torch\n",
        "print(f\"\\nEnvironment verification:\")\n",
        "print(f\"Python version: {sys.version}\")\n",
        "print(f\"PyTorch version: {torch.__version__}\")\n",
        "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"CUDA version: {torch.version.cuda}\")\n",
        "    print(f\"GPU count: {torch.cuda.device_count()}\")\n",
        "    for i in range(torch.cuda.device_count()):\n",
        "        print(f\"  GPU {i}: {torch.cuda.get_device_name(i)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### **Project Path Setup**\n",
        "Sets up local project paths for Lambda Labs execution."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lr7jpBx7z79b",
        "outputId": "aeca7557-5812-42ca-816a-6a6c21cd9a5d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Project directory: /home/ubuntu/HallucinationVectorProject\n",
            "Data directory: /home/ubuntu/HallucinationVectorProject/data\n",
            "Artifacts directory: /home/ubuntu/HallucinationVectorProject/artifacts/llama-3.1-8b\n",
            "Results directory: /home/ubuntu/HallucinationVectorProject/results/llama-3.1-8b\n",
            "âœ“ Environment configured for local Lambda Labs execution.\n"
          ]
        }
      ],
      "source": [
        "import sys\n",
        "import os\n",
        "from pathlib import Path\n",
        "\n",
        "# Setup local project paths\n",
        "PROJECT_DIR = Path(\"/home/ubuntu/HallucinationVectorProject\")\n",
        "DATA_DIR = PROJECT_DIR / \"data\"\n",
        "ARTIFACTS_DIR = PROJECT_DIR / \"artifacts\" / \"llama-3.1-8b\"\n",
        "RESULTS_DIR = PROJECT_DIR / \"results\" / \"llama-3.1-8b\"\n",
        "\n",
        "# Create directories if they don't exist\n",
        "ARTIFACTS_DIR.mkdir(parents=True, exist_ok=True)\n",
        "RESULTS_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# Add project directory to Python's path\n",
        "project_path = str(PROJECT_DIR)\n",
        "if project_path not in sys.path:\n",
        "    sys.path.append(project_path)\n",
        "\n",
        "print(f\"Project directory: {PROJECT_DIR}\")\n",
        "print(f\"Data directory: {DATA_DIR}\")\n",
        "print(f\"Artifacts directory: {ARTIFACTS_DIR}\")\n",
        "print(f\"Results directory: {RESULTS_DIR}\")\n",
        "\n",
        "# Programmatically set the environment to 'local' in the config file\n",
        "config_file_path = PROJECT_DIR / 'config.py'\n",
        "with open(config_file_path, 'r') as f:\n",
        "    lines = f.readlines()\n",
        "with open(config_file_path, 'w') as f:\n",
        "    for line in lines:\n",
        "        if line.strip().startswith('ENVIRONMENT ='):\n",
        "            f.write('ENVIRONMENT = \"local\"\\n')\n",
        "        else:\n",
        "            f.write(line)\n",
        "print(\"âœ“ Environment configured for local Lambda Labs execution.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### **Load Artifacts and Model Setup**\n",
        "Loads all required model artifacts for Llama-3.1-8B, including the hallucination vector, risk classifier, and config thresholds, and prepares the model for evaluation on A100 40GB GPU."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 373,
          "referenced_widgets": [
            "b0f86b759fa34618840fd7f6426ca31b",
            "b2c0965a7c8f4e06841bc97baf6913cc",
            "8b98fbbe2d51452aa3605cc56fd45703",
            "5937ff2ff09145a1aed178187eec6b11",
            "1ad470b0a62d4601a1519bae23982985",
            "8bfec65739ab4b2f8c11a995172f3623",
            "4f5b35cbc6774df3b1fe397b14640933",
            "33b418ea3b974d788e1b0e736ae59414",
            "85efeb56ef7b4ec2b69753e2d8d16abb",
            "5848feacc80a4506acd04c7ab328720c",
            "6af94307d3504f029600735742ec2ba8",
            "b2bf26fd9bef4f3da213f856ee74260c",
            "de91c5159db443b0bf0caf88e72d312b",
            "a2929e498fd2482681c9ab3dd7a18856",
            "2b09d3f2094d4906a56883d3f467bcba",
            "79cbba6bd54549bd8131cd887d2e0708",
            "12bd09bfc4d64f2897444365ae8fcf55",
            "905c490e14704d99a8050dcdfea8a4aa",
            "a4a1ed5428f14053b589bd5760905c23",
            "eb9d8be9ae874961b365535a32d3bdfa",
            "cc268fe08c5649c6a172c887ea637b8e",
            "baffd1a874b84e69bf0dabefffa6fa5c",
            "fb9418c321b74ca2905264a7015a3cf7",
            "39c05fc5cece43c696cb21be0d7598fb",
            "0a130b649922465fbaaa39360bc182b3",
            "1ce0fe58bdf04419bba04b24bb6b3d44",
            "ccec744d07344082beaad09c8e756b59",
            "0af727690a8c45189de6b891f5544520",
            "9ef056edfefb42859c1017479e729f8a",
            "0ff04e29983d44f4b0500bd187413494",
            "fb5f686ae0d846daa92a3f7eca0cb09c",
            "390a34c1d7c44413888da54078fed4ee",
            "f757bbf5954e4ad093e50081ce4d4187",
            "b1caba83cb704bd98a96128eece691e0",
            "93dc8d4505ff405585edcf873831fccb",
            "bb2b2b8149da46c2909a9de140c1be37",
            "5a8876aa9160455aaace1cff34d3053d",
            "e2f0f384cb344d49af8c1eb063f0c370",
            "21b5555a566947a395837924a0425748",
            "ce416e14219d473aa127531c4e806b0f",
            "79ee4dd2310a4113b30d00dd7b9f42e9",
            "2a8f6933fd824103be3ae29cfb4aff07",
            "853eae85943e4f059bb4f284ab03857a",
            "27cd1ec6893242ceade77c811d0f4c20",
            "1cc59559f4714bb68164d44fbe9c4388",
            "58e1ff15b9af4bab8225b462de199ef1",
            "d9ebdf759a5347609c1ae2f30d7a73a1",
            "16c66f714ed4455d98d2d908aac474e9",
            "659934e23afd4d2ba94f672058a226c8",
            "92f6b3ced073463089b038f2bc506e87",
            "f104dba960124583a9e01cdedb77fec2",
            "ee561b9b6117400b80def264d5cc0c5a",
            "59ec59c5eb22439989621785442232a3",
            "0ed0d6557757456b9e17a68f962a6101",
            "7085b78429354064a9fab9ce02859ab1"
          ]
        },
        "id": "X7eAPnlB0VVQ",
        "outputId": "235bd42a-a834-4e6b-84ff-4fd49bf52193"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ðŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[xformers|WARNING]WARNING[XFORMERS]: xFormers can't load C++/CUDA extensions. xFormers was built for:\n",
            "    PyTorch 2.9.0+cu130 with CUDA 1300 (you have 2.9.0+cu128)\n",
            "    Python  3.12.12 (you have 3.10.12)\n",
            "  Please reinstall xformers (see https://github.com/facebookresearch/xformers#installing-xformers)\n",
            "  Memory-efficient attention, SwiGLU, sparse and more won't be available.\n",
            "  Set XFORMERS_MORE_DETAILS=1 for more details\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "========\n",
            "Switching to PyTorch attention since your Xformers is broken.\n",
            "========\n",
            "\n",
            "Unsloth: Xformers was not installed correctly.\n",
            "Please install xformers separately first.\n",
            "Then confirm if it's correctly installed by running:\n",
            "python -m xformers.info\n",
            "\n",
            "Longer error message:\n",
            "xFormers can't load C++/CUDA extensions. xFormers was built for:\n",
            "    PyTorch 2.9.0+cu130 with CUDA 1300 (you have 2.9.0+cu128)\n",
            "    Python  3.12.12 (you have 3.10.12)\n",
            "  Please reinstall xformers (see https://github.com/facebookresearch/xformers#installing-xformers)\n",
            "  Memory-efficient attention, SwiGLU, sparse and more won't be available.\n",
            "ðŸ¦¥ Unsloth Zoo will now patch everything to make training faster!\n",
            "Loading all necessary artifacts for Llama-3.1-8B evaluation...\n",
            "\n",
            "GPU memory before model loading:\n",
            "GPU 0 (NVIDIA A100-SXM4-40GB): 0.00GB allocated, 0.00GB reserved, 39.49GB total\n",
            "\n",
            "Loading Llama-3.1-8B model (bfloat16)...\n",
            "Loading all necessary artifacts for Llama-3.1-8B evaluation...\n",
            "\n",
            "GPU memory before model loading:\n",
            "GPU 0 (NVIDIA A100-SXM4-40GB): 0.00GB allocated, 0.00GB reserved, 39.49GB total\n",
            "\n",
            "Loading Llama-3.1-8B model (bfloat16)...\n",
            "Unsloth: WARNING `trust_remote_code` is True.\n",
            "Are you certain you want to do remote code execution?\n",
            "==((====))==  Unsloth 2025.10.11: Fast Llama patching. Transformers: 4.57.1.\n",
            "   \\\\   /|    NVIDIA A100-SXM4-40GB. Num GPUs = 1. Max memory: 39.495 GB. Platform: Linux.\n",
            "O^O/ \\_/ \\    Torch: 2.9.0+cu128. CUDA: 8.0. CUDA Toolkit: 12.8. Triton: 3.5.0\n",
            "\\        /    Bfloat16 = TRUE. FA [Xformers = None. FA2 = False]\n",
            " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
            "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n",
            "Unsloth: WARNING `trust_remote_code` is True.\n",
            "Are you certain you want to do remote code execution?\n",
            "==((====))==  Unsloth 2025.10.11: Fast Llama patching. Transformers: 4.57.1.\n",
            "   \\\\   /|    NVIDIA A100-SXM4-40GB. Num GPUs = 1. Max memory: 39.495 GB. Platform: Linux.\n",
            "O^O/ \\_/ \\    Torch: 2.9.0+cu128. CUDA: 8.0. CUDA Toolkit: 12.8. Triton: 3.5.0\n",
            "\\        /    Bfloat16 = TRUE. FA [Xformers = None. FA2 = False]\n",
            " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
            "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "3d93783b7203408e85ff12949ed9e545",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "âœ“ All artifacts loaded successfully!\n",
            "Model device: cuda:0\n",
            "\n",
            "GPU memory after loading:\n",
            "GPU 0 (NVIDIA A100-SXM4-40GB): 14.98GB allocated, 15.09GB reserved, 39.49GB total\n"
          ]
        }
      ],
      "source": [
        "import time\n",
        "import pandas as pd\n",
        "import torch\n",
        "import csv\n",
        "import sys\n",
        "from pathlib import Path\n",
        "from tqdm import tqdm\n",
        "import joblib\n",
        "from unsloth import FastLanguageModel\n",
        "\n",
        "# Ensure project directory is in path before importing custom modules\n",
        "PROJECT_DIR = Path(\"/home/ubuntu/HallucinationVectorProject\")\n",
        "if str(PROJECT_DIR) not in sys.path:\n",
        "    sys.path.insert(0, str(PROJECT_DIR))\n",
        "\n",
        "# Import custom modules\n",
        "\n",
        "\n",
        "os.environ[\"UNSLOTH_STABLE_DOWNLOADS\"] = \"1\"\n",
        "\n",
        "\n",
        "# Helper function to monitor GPU memory\n",
        "def print_gpu_memory():\n",
        "    \"\"\"Print memory usage for all available GPUs.\"\"\"\n",
        "    if torch.cuda.is_available():\n",
        "        allocated = torch.cuda.memory_allocated(0) / 1024**3\n",
        "        reserved = torch.cuda.memory_reserved(0) / 1024**3\n",
        "        total = torch.cuda.get_device_properties(0).total_memory / 1024**3\n",
        "        print(f\"GPU 0 ({torch.cuda.get_device_name(0)}): \"\n",
        "              f\"{allocated:.2f}GB allocated, {reserved:.2f}GB reserved, {total:.2f}GB total\")\n",
        "\n",
        "# This global dictionary will hold our models, tokenizer, vectors, etc.\n",
        "artifacts = {}\n",
        "\n",
        "def load_all_artifacts():\n",
        "    \"\"\"Loads all necessary model and project artifacts into the global dict.\"\"\"\n",
        "    if artifacts: return\n",
        "    print(\"Loading all necessary artifacts for Llama-3.1-8B evaluation...\")\n",
        "    \n",
        "    print(\"\\nGPU memory before model loading:\")\n",
        "    print_gpu_memory()\n",
        "    \n",
        "    # Clear any cached memory\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.empty_cache()\n",
        "    \n",
        "    # Load 8B model for single GPU\n",
        "    print(\"\\nLoading Llama-3.1-8B model (bfloat16)...\")\n",
        "    model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "        model_name=\"unsloth/Meta-Llama-3.1-8B-Instruct\",\n",
        "        max_seq_length=4096,\n",
        "        dtype=torch.bfloat16,\n",
        "        load_in_4bit=False,\n",
        "        trust_remote_code=True,\n",
        "    )\n",
        "\n",
        "    # Configure for inference\n",
        "    model = FastLanguageModel.for_inference(model)\n",
        "    model.gradient_checkpointing_disable()\n",
        "    model.config.gradient_checkpointing = False\n",
        "    model.config.use_cache = True\n",
        "    model.eval()\n",
        "\n",
        "    artifacts['model'] = model\n",
        "    artifacts['tokenizer'] = tokenizer\n",
        "    artifacts['v_halluc'] = torch.load(ARTIFACTS_DIR / \"v_halluc.pt\").to(model.device).to(torch.bfloat16)\n",
        "    artifacts['risk_classifier'] = joblib.load(ARTIFACTS_DIR / \"risk_clf.joblib\")\n",
        "    artifacts['thresholds'] = {\n",
        "        \"tau_low\": 0.0390,\n",
        "        \"tau_high\": 0.0547,\n",
        "        \"optimal_alpha\": -1.0\n",
        "    }\n",
        "    \n",
        "    print(\"\\nâœ“ All artifacts loaded successfully!\")\n",
        "    print(f\"Model device: {model.device}\")\n",
        "    print(\"\\nGPU memory after loading:\")\n",
        "    print_gpu_memory()\n",
        "\n",
        "# Load everything\n",
        "load_all_artifacts()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### **Combined Selective Activation Steering and Guardrail Function**\n",
        "Defines a context manager to apply the steering vector only to the first N tokens, with dynamic risk-proportional strength, and a function to generate answers using this combined intervention."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mKmDgmbl0hW5",
        "outputId": "f5a0c6f6-71ec-4e9b-c944-43b2683fef08"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Defining combined logic for 'Dynamic Alpha + Selective Steering' experiment...\n",
            "âœ“ New function `answer_guarded_combined` is now defined and ready for the experiment.\n"
          ]
        }
      ],
      "source": [
        "import time\n",
        "import torch\n",
        "import sys\n",
        "from pathlib import Path\n",
        "from contextlib import contextmanager\n",
        "\n",
        "# Ensure project directory is in path before importing custom modules\n",
        "PROJECT_DIR = Path(\"/home/ubuntu/HallucinationVectorProject\")\n",
        "if str(PROJECT_DIR) not in sys.path:\n",
        "    sys.path.insert(0, str(PROJECT_DIR))\n",
        "\n",
        "# Import our project's config and utils modules\n",
        "import config\n",
        "import utils\n",
        "\n",
        "print(\"Defining combined logic for 'Dynamic Alpha + Selective Steering' experiment...\")\n",
        "\n",
        "# --- 1. The SelectiveActivationSteerer Class ---\n",
        "\n",
        "class SelectiveActivationSteerer:\n",
        "    def __init__(self, model, steering_vector, layer_idx, coeff=1.0, steering_token_limit=10):\n",
        "        self.model = model\n",
        "        self.vector = steering_vector\n",
        "        self.layer_idx = layer_idx\n",
        "        self.coeff = coeff\n",
        "        self.steering_token_limit = steering_token_limit\n",
        "        self._handle = None\n",
        "        self._layer_path = f\"model.layers.{self.layer_idx}\"\n",
        "        self.call_count = 0\n",
        "\n",
        "    def _hook_fn(self, module, ins, out):\n",
        "        self.call_count += 1\n",
        "        if self.call_count <= self.steering_token_limit:\n",
        "            steered_output = out[0] + (self.coeff * self.vector.to(out[0].device))\n",
        "            return (steered_output,) + out[1:]\n",
        "        return out\n",
        "\n",
        "    def __enter__(self):\n",
        "        self.call_count = 0\n",
        "        try:\n",
        "            layer = self.model.get_submodule(self._layer_path)\n",
        "            self._handle = layer.register_forward_hook(self._hook_fn)\n",
        "        except AttributeError:\n",
        "            raise AttributeError(f\"Could not find the layer at path: {self._layer_path}\")\n",
        "        return self\n",
        "\n",
        "    def __exit__(self, exc_type, exc_val, exc_tb):\n",
        "        if self._handle:\n",
        "            self._handle.remove()\n",
        "\n",
        "\n",
        "# --- 2. The New `answer_guarded_combined` Function ---\n",
        "# This function combines the logic from both successful ablations.\n",
        "\n",
        "def answer_guarded_combined(prompt_text: str, max_new_tokens: int = 128, steering_token_limit: int = 10):\n",
        "    \"\"\"\n",
        "    Generates a response using the guardrail with DYNAMIC alpha and SELECTIVE steering.\n",
        "    Enhanced for 70B model with proper device handling and memory management.\n",
        "    \"\"\"\n",
        "    start_time = time.time()\n",
        "\n",
        "    try:\n",
        "        risk_score = utils.get_hallucination_risk(\n",
        "            prompt_text, artifacts['model'], artifacts['tokenizer'],\n",
        "            artifacts['v_halluc'], artifacts['risk_classifier']\n",
        "        )\n",
        "\n",
        "        full_prompt = f\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\nYou are a helpful assistant.\\n<|eot_id|><|start_header_id|>user<|end_header_id|>\\nAnswer the following briefly.\\n{prompt_text}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n\"\n",
        "        inputs = artifacts['tokenizer'](full_prompt, return_tensors=\"pt\", max_length=4096, truncation=True).to(artifacts['model'].device)\n",
        "        input_token_length = inputs.input_ids.shape[1]\n",
        "\n",
        "        if risk_score < artifacts['thresholds']['tau_high']:\n",
        "            path = \"Fast Path (Untouched)\"\n",
        "            with torch.no_grad():\n",
        "                outputs = artifacts['model'].generate(\n",
        "                    **inputs, \n",
        "                    max_new_tokens=max_new_tokens, \n",
        "                    do_sample=False,\n",
        "                    pad_token_id=artifacts['tokenizer'].eos_token_id\n",
        "                )\n",
        "        else:\n",
        "            # From Dynamic Alpha Ablation: Calculate dynamic steering strength.\n",
        "            optimal_alpha = artifacts['thresholds']['optimal_alpha']\n",
        "            tau_high = artifacts['thresholds']['tau_high']\n",
        "            scaling_factor = (risk_score - tau_high) / (1.0 - tau_high + 1e-6) # Add epsilon for stability\n",
        "            dynamic_alpha = optimal_alpha * max(0, min(1, scaling_factor)) # Clamp between 0 and 1\n",
        "\n",
        "            path = f\"Combined Steer Path (Î±={dynamic_alpha:.2f}, N={steering_token_limit})\"\n",
        "\n",
        "            # From Selective N-Tokens Ablation: Use the steerer with a token limit.\n",
        "            # We pass our newly calculated `dynamic_alpha` as the coefficient.\n",
        "            with SelectiveActivationSteerer(\n",
        "                artifacts['model'], artifacts['v_halluc'], config.TARGET_LAYER,\n",
        "                coeff=dynamic_alpha,\n",
        "                steering_token_limit=steering_token_limit\n",
        "            ):\n",
        "                with torch.no_grad():\n",
        "                    outputs = artifacts['model'].generate(\n",
        "                        **inputs, \n",
        "                        max_new_tokens=max_new_tokens, \n",
        "                        do_sample=False,\n",
        "                        pad_token_id=artifacts['tokenizer'].eos_token_id\n",
        "                    )\n",
        "\n",
        "        answer = artifacts['tokenizer'].decode(outputs[0, input_token_length:], skip_special_tokens=True)\n",
        "        latency = time.time() - start_time\n",
        "\n",
        "        return {\"answer\": answer.strip(), \"risk_score\": risk_score, \"path_taken\": path, \"latency_seconds\": latency}\n",
        "    \n",
        "    except Exception as e:\n",
        "        print(f\"Error in answer_guarded_combined: {e}\")\n",
        "        latency = time.time() - start_time\n",
        "        return {\"answer\": \"\", \"risk_score\": 0.5, \"path_taken\": \"Error\", \"latency_seconds\": latency}\n",
        "\n",
        "print(\"âœ“ New function `answer_guarded_combined` is now defined and ready for the experiment.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Suppress Warnings**\n",
        "Suppresses specific sklearn warnings for cleaner output during evaluation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "OZPn3nOm5gLa"
      },
      "outputs": [],
      "source": [
        "import warnings\n",
        "\n",
        "warnings.filterwarnings(\n",
        "    \"ignore\",\n",
        "    message=\"X does not have valid feature names\",\n",
        "    category=UserWarning,\n",
        "    module=\"sklearn\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### **Run Combined Guardrail Evaluation**\n",
        "Runs the evaluation loop on the TruthfulQA test set, applying the combined guardrail and saving results for each prompt."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eHzIZVAI12fr",
        "outputId": "fb931eeb-a1b1-4332-8512-79a85cb82381"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "New guarded results will be saved to: /home/ubuntu/HallucinationVectorProject/results/llama-3.1-8b/combined_guarded_results.csv\n",
            "Loaded 617 test prompts from TruthfulQA\n",
            "Starting response generation for combined guardrail (Dynamic Alpha + Selective N=10)...\n",
            "Already processed: 173 prompts\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Combined Guardrail Evaluation:   0%|          | 0/617 [00:00<?, ?it/s]"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Combined Guardrail Evaluation:  28%|â–ˆâ–ˆâ–Š       | 174/617 [00:01<00:04, 108.87it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Progress: 180/617 (29.2%) | Rate: 12.68 prompts/s | ETA: 0.6 min\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Combined Guardrail Evaluation:  30%|â–ˆâ–ˆâ–ˆ       | 188/617 [00:31<02:24,  2.97it/s] "
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Progress: 190/617 (30.8%) | Rate: 5.07 prompts/s | ETA: 1.4 min\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Combined Guardrail Evaluation:  32%|â–ˆâ–ˆâ–ˆâ–      | 200/617 [00:54<06:46,  1.03it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Progress: 200/617 (32.4%) | Rate: 3.70 prompts/s | ETA: 1.9 min\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Combined Guardrail Evaluation:  34%|â–ˆâ–ˆâ–ˆâ–      | 210/617 [01:08<11:01,  1.63s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Progress: 210/617 (34.0%) | Rate: 3.09 prompts/s | ETA: 2.2 min\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Combined Guardrail Evaluation:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 220/617 [01:27<16:49,  2.54s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Progress: 220/617 (35.7%) | Rate: 2.51 prompts/s | ETA: 2.6 min\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Combined Guardrail Evaluation:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 230/617 [01:46<15:14,  2.36s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Progress: 230/617 (37.3%) | Rate: 2.16 prompts/s | ETA: 3.0 min\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Combined Guardrail Evaluation:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 240/617 [02:07<14:14,  2.27s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Progress: 240/617 (38.9%) | Rate: 1.89 prompts/s | ETA: 3.3 min\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Combined Guardrail Evaluation:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 250/617 [02:23<11:25,  1.87s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Progress: 250/617 (40.5%) | Rate: 1.74 prompts/s | ETA: 3.5 min\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Combined Guardrail Evaluation:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 260/617 [02:42<11:38,  1.96s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Progress: 260/617 (42.1%) | Rate: 1.60 prompts/s | ETA: 3.7 min\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Combined Guardrail Evaluation:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 270/617 [02:59<11:55,  2.06s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Progress: 270/617 (43.8%) | Rate: 1.50 prompts/s | ETA: 3.8 min\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Combined Guardrail Evaluation:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 280/617 [03:15<07:12,  1.28s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Progress: 280/617 (45.4%) | Rate: 1.43 prompts/s | ETA: 3.9 min\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Combined Guardrail Evaluation:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 290/617 [03:31<12:27,  2.28s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Progress: 290/617 (47.0%) | Rate: 1.37 prompts/s | ETA: 4.0 min\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Combined Guardrail Evaluation:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 300/617 [03:47<12:24,  2.35s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Progress: 300/617 (48.6%) | Rate: 1.32 prompts/s | ETA: 4.0 min\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Combined Guardrail Evaluation:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 310/617 [04:03<10:10,  1.99s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Progress: 310/617 (50.2%) | Rate: 1.27 prompts/s | ETA: 4.0 min\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Combined Guardrail Evaluation:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 320/617 [04:25<09:13,  1.87s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Progress: 320/617 (51.9%) | Rate: 1.21 prompts/s | ETA: 4.1 min\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Combined Guardrail Evaluation:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 330/617 [04:46<10:28,  2.19s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Progress: 330/617 (53.5%) | Rate: 1.15 prompts/s | ETA: 4.2 min\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Combined Guardrail Evaluation:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 340/617 [05:09<13:09,  2.85s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Progress: 340/617 (55.1%) | Rate: 1.10 prompts/s | ETA: 4.2 min\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Combined Guardrail Evaluation:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 350/617 [05:28<10:58,  2.47s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Progress: 350/617 (56.7%) | Rate: 1.07 prompts/s | ETA: 4.2 min\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Combined Guardrail Evaluation:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 360/617 [05:37<02:58,  1.44it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Progress: 360/617 (58.3%) | Rate: 1.07 prompts/s | ETA: 4.0 min\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Combined Guardrail Evaluation:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 370/617 [05:53<04:47,  1.16s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Progress: 370/617 (60.0%) | Rate: 1.05 prompts/s | ETA: 3.9 min\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Combined Guardrail Evaluation:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 380/617 [06:19<11:07,  2.82s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Progress: 380/617 (61.6%) | Rate: 1.00 prompts/s | ETA: 3.9 min\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Combined Guardrail Evaluation:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 391/617 [06:33<05:18,  1.41s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Progress: 390/617 (63.2%) | Rate: 0.99 prompts/s | ETA: 3.8 min\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Combined Guardrail Evaluation:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 400/617 [06:53<08:09,  2.26s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Progress: 400/617 (64.8%) | Rate: 0.97 prompts/s | ETA: 3.7 min\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Combined Guardrail Evaluation:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 410/617 [07:11<06:54,  2.00s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Progress: 410/617 (66.5%) | Rate: 0.95 prompts/s | ETA: 3.6 min\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Combined Guardrail Evaluation:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 420/617 [07:29<04:29,  1.37s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Progress: 420/617 (68.1%) | Rate: 0.94 prompts/s | ETA: 3.5 min\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Combined Guardrail Evaluation:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 430/617 [07:46<03:46,  1.21s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Progress: 430/617 (69.7%) | Rate: 0.92 prompts/s | ETA: 3.4 min\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Combined Guardrail Evaluation:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 440/617 [08:01<03:30,  1.19s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Progress: 440/617 (71.3%) | Rate: 0.91 prompts/s | ETA: 3.2 min\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Combined Guardrail Evaluation:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 450/617 [08:26<07:15,  2.61s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Progress: 450/617 (72.9%) | Rate: 0.89 prompts/s | ETA: 3.1 min\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Combined Guardrail Evaluation:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 460/617 [08:51<05:11,  1.98s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Progress: 460/617 (74.6%) | Rate: 0.87 prompts/s | ETA: 3.0 min\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Combined Guardrail Evaluation:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 470/617 [09:16<07:13,  2.95s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Progress: 470/617 (76.2%) | Rate: 0.85 prompts/s | ETA: 2.9 min\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Combined Guardrail Evaluation:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 480/617 [09:33<04:29,  1.97s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Progress: 480/617 (77.8%) | Rate: 0.84 prompts/s | ETA: 2.7 min\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Combined Guardrail Evaluation:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 490/617 [09:51<04:20,  2.05s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Progress: 490/617 (79.4%) | Rate: 0.83 prompts/s | ETA: 2.6 min\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Combined Guardrail Evaluation:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 500/617 [10:21<05:47,  2.97s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Progress: 500/617 (81.0%) | Rate: 0.80 prompts/s | ETA: 2.4 min\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Combined Guardrail Evaluation:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 510/617 [10:43<04:15,  2.39s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Progress: 510/617 (82.7%) | Rate: 0.79 prompts/s | ETA: 2.3 min\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Combined Guardrail Evaluation:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 520/617 [11:08<04:28,  2.77s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Progress: 520/617 (84.3%) | Rate: 0.78 prompts/s | ETA: 2.1 min\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Combined Guardrail Evaluation:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 530/617 [11:23<02:25,  1.67s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Progress: 530/617 (85.9%) | Rate: 0.78 prompts/s | ETA: 1.9 min\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Combined Guardrail Evaluation:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 540/617 [11:49<03:14,  2.52s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Progress: 540/617 (87.5%) | Rate: 0.76 prompts/s | ETA: 1.7 min\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Combined Guardrail Evaluation:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 550/617 [12:04<01:24,  1.26s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Progress: 550/617 (89.1%) | Rate: 0.76 prompts/s | ETA: 1.5 min\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Combined Guardrail Evaluation:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 560/617 [12:23<01:27,  1.54s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Progress: 560/617 (90.8%) | Rate: 0.75 prompts/s | ETA: 1.3 min\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Combined Guardrail Evaluation:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 570/617 [12:48<02:01,  2.58s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Progress: 570/617 (92.4%) | Rate: 0.74 prompts/s | ETA: 1.1 min\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Combined Guardrail Evaluation:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 580/617 [13:14<01:24,  2.27s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Progress: 580/617 (94.0%) | Rate: 0.73 prompts/s | ETA: 0.8 min\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Combined Guardrail Evaluation:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 590/617 [13:31<00:40,  1.50s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Progress: 590/617 (95.6%) | Rate: 0.73 prompts/s | ETA: 0.6 min\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Combined Guardrail Evaluation:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 600/617 [13:49<00:25,  1.50s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Progress: 600/617 (97.2%) | Rate: 0.72 prompts/s | ETA: 0.4 min\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Combined Guardrail Evaluation:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 610/617 [14:09<00:08,  1.18s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Progress: 610/617 (98.9%) | Rate: 0.72 prompts/s | ETA: 0.2 min\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Combined Guardrail Evaluation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 617/617 [14:23<00:00,  1.40s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "âœ“ Combined guardrail evaluation complete in 14.39 minutes\n",
            "Results saved to: /home/ubuntu/HallucinationVectorProject/results/llama-3.1-8b/combined_guarded_results.csv\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# --- EXPERIMENT PARAMETER ---\n",
        "STEERING_TOKEN_LIMIT = 10 # The 'N' for our selective steering\n",
        "\n",
        "# Use local paths\n",
        "GUARDED_RESULTS_PATH_COMBINED = RESULTS_DIR / \"combined_guarded_results.csv\"\n",
        "BASELINE_RESULTS_PATH = RESULTS_DIR / \"ablation_2_baseline_results_truthfulqa.csv\"\n",
        "\n",
        "print(f\"New guarded results will be saved to: {GUARDED_RESULTS_PATH_COMBINED}\")\n",
        "\n",
        "# Memory management helper\n",
        "def check_and_clear_memory(threshold_gb=60):\n",
        "    \"\"\"Clear GPU cache if memory usage exceeds threshold.\"\"\"\n",
        "    if torch.cuda.is_available():\n",
        "        for i in range(torch.cuda.device_count()):\n",
        "            allocated = torch.cuda.memory_allocated(i) / 1024**3\n",
        "            if allocated > threshold_gb:\n",
        "                print(f\"GPU {i} memory ({allocated:.2f}GB) exceeds threshold. Clearing cache...\")\n",
        "                torch.cuda.empty_cache()\n",
        "                return True\n",
        "    return False\n",
        "\n",
        "# Load the test set (using local path)\n",
        "test_df = pd.read_csv(DATA_DIR / \"final_test_set_truthfulqa.csv\")\n",
        "print(f\"Loaded {len(test_df)} test prompts from TruthfulQA\")\n",
        "\n",
        "# --- Resilient Evaluation Loop ---\n",
        "guarded_headers = ['prompt', 'answer', 'risk_score', 'path_taken', 'latency_seconds']\n",
        "utils.initialize_csv(GUARDED_RESULTS_PATH_COMBINED, guarded_headers)\n",
        "\n",
        "processed_guarded = utils.load_processed_prompts(GUARDED_RESULTS_PATH_COMBINED)\n",
        "\n",
        "print(f\"Starting response generation for combined guardrail (Dynamic Alpha + Selective N={STEERING_TOKEN_LIMIT})...\")\n",
        "print(f\"Already processed: {len(processed_guarded)} prompts\")\n",
        "\n",
        "start_time = time.time()\n",
        "processed_count = len(processed_guarded)\n",
        "\n",
        "for idx, row in tqdm(test_df.iterrows(), total=len(test_df), desc=\"Combined Guardrail Evaluation\"):\n",
        "    prompt = row['Question']\n",
        "\n",
        "    # Guarded Run\n",
        "    if prompt not in processed_guarded:\n",
        "        try:\n",
        "            result = answer_guarded_combined(prompt, steering_token_limit=STEERING_TOKEN_LIMIT)\n",
        "            with open(GUARDED_RESULTS_PATH_COMBINED, 'a', newline='', encoding='utf-8') as f:\n",
        "                csv.writer(f).writerow([prompt] + list(result.values()))\n",
        "            \n",
        "            processed_count += 1\n",
        "            \n",
        "            # Progress tracking and memory management\n",
        "            if processed_count % 10 == 0:\n",
        "                elapsed = time.time() - start_time\n",
        "                rate = processed_count / elapsed if elapsed > 0 else 0\n",
        "                remaining = len(test_df) - processed_count\n",
        "                eta = remaining / rate if rate > 0 else 0\n",
        "                print(f\"Progress: {processed_count}/{len(test_df)} ({processed_count/len(test_df)*100:.1f}%) | \"\n",
        "                      f\"Rate: {rate:.2f} prompts/s | ETA: {eta/60:.1f} min\")\n",
        "                check_and_clear_memory()\n",
        "                \n",
        "        except Exception as e:\n",
        "            print(f\"Error on guarded prompt: {prompt[:50]}... Error: {e}\")\n",
        "\n",
        "print(f\"\\nâœ“ Combined guardrail evaluation complete in {(time.time() - start_time)/60:.2f} minutes\")\n",
        "print(f\"Results saved to: {GUARDED_RESULTS_PATH_COMBINED}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Define the Baseline Generation Function\n",
        "This function will generate an answer from the original, unguarded model. It includes the requested prompt prefix and decodes only the new tokens."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Baseline generation function `generate_baseline` is defined.\n"
          ]
        }
      ],
      "source": [
        "import time\n",
        "\n",
        "def generate_baseline(prompt_text: str, max_new_tokens: int = 128):\n",
        "    \"\"\"\n",
        "    Generates a response from the unguarded baseline model.\n",
        "    \"\"\"\n",
        "    start_time = time.time()\n",
        "\n",
        "    # Prepend the required instruction to the prompt\n",
        "    full_prompt = f\"<|begin_of_text|><|start_header_id|>system<|end_header_id|> You are a helpful AI assistant. <|eot_id|><|start_header_id|>user<|end_header_id|> Answer the following question briefly:\\n{prompt_text} <|eot_id|><|start_header_id|>assistant<|end_header_id|>\"\n",
        "\n",
        "    inputs = artifacts['tokenizer'](full_prompt, return_tensors=\"pt\").to(artifacts['model'].device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = artifacts['model'].generate(\n",
        "            **inputs,\n",
        "            max_new_tokens=max_new_tokens,\n",
        "            do_sample=False,\n",
        "            pad_token_id=artifacts['tokenizer'].eos_token_id\n",
        "        )\n",
        "\n",
        "    # Decode only the newly generated tokens, skipping the prompt\n",
        "    input_token_length = inputs.input_ids.shape[1]\n",
        "    newly_generated_tokens = outputs[0, input_token_length:]\n",
        "    answer = artifacts['tokenizer'].decode(newly_generated_tokens, skip_special_tokens=True)\n",
        "\n",
        "    end_time = time.time()\n",
        "    latency = end_time - start_time\n",
        "\n",
        "    return {\n",
        "        \"answer\": answer,\n",
        "        \"latency_seconds\": latency\n",
        "    }\n",
        "\n",
        "print(\"Baseline generation function `generate_baseline` is defined.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Baseline results will be saved to: /home/ubuntu/HallucinationVectorProject/baseline_results_truthfulqa.csv\n",
            "Found 0 already processed prompts for the Baseline run.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Evaluating Prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 617/617 [22:59<00:00,  2.24s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--- Evaluation Runs Complete ---\n",
            "Total baseline results saved: 617\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "BASELINE_RESULTS_PATH = os.path.join(PROJECT_DIR, \"baseline_results_truthfulqa.csv\")\n",
        "\n",
        "print(f\"Baseline results will be saved to: {BASELINE_RESULTS_PATH}\")\n",
        "\n",
        "# --- Helper function to initialize CSV files with headers ---\n",
        "def initialize_csv(file_path, headers):\n",
        "    if not os.path.exists(file_path):\n",
        "        with open(file_path, 'w', newline='', encoding='utf-8') as f:\n",
        "            writer = csv.writer(f)\n",
        "            writer.writerow(headers)\n",
        "        return set()\n",
        "    else:\n",
        "        # Load existing prompts to know where to resume from\n",
        "        df_existing = pd.read_csv(file_path)\n",
        "        return set(df_existing['prompt'].tolist())\n",
        "\n",
        "# --- Initialize CSVs and get the set of already processed prompts ---\n",
        "baseline_headers = ['prompt', 'answer', 'latency_seconds']\n",
        "\n",
        "processed_baseline = initialize_csv(BASELINE_RESULTS_PATH, baseline_headers)\n",
        "\n",
        "print(f\"Found {len(processed_baseline)} already processed prompts for the Baseline run.\")\n",
        "\n",
        "# --- Main Evaluation Loop ---\n",
        "if test_df is not None:\n",
        "    # Use tqdm for a progress bar, which will handle progress indication\n",
        "    for index, row in tqdm(test_df.iterrows(), total=len(test_df), desc=\"Evaluating Prompts\"):\n",
        "        prompt = row['Question']\n",
        "\n",
        "        # --- Baseline Run ---\n",
        "        if prompt not in processed_baseline:\n",
        "            try:\n",
        "                baseline_result = generate_baseline(prompt)\n",
        "\n",
        "                # Append result immediately to the CSV\n",
        "                with open(BASELINE_RESULTS_PATH, 'a', newline='', encoding='utf-8') as f:\n",
        "                    writer = csv.writer(f)\n",
        "                    writer.writerow([\n",
        "                        prompt,\n",
        "                        baseline_result.get('answer'),\n",
        "                        baseline_result.get('latency_seconds')\n",
        "                    ])\n",
        "                processed_baseline.add(prompt)\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"  -> ERROR during baseline run for prompt: '{prompt}'. Error: {e}\")\n",
        "                with open(BASELINE_RESULTS_PATH, 'a', newline='', encoding='utf-8') as f:\n",
        "                    writer = csv.writer(f)\n",
        "                    writer.writerow([prompt, f\"ERROR: {e}\", -1.0])\n",
        "                processed_baseline.add(prompt)\n",
        "\n",
        "    print(\"\\n--- Evaluation Runs Complete ---\")\n",
        "    # Final check\n",
        "    final_baseline_df = pd.read_csv(BASELINE_RESULTS_PATH)\n",
        "    print(f\"Total baseline results saved: {len(final_baseline_df)}\")\n",
        "else:\n",
        "    print(\"Skipping evaluation loop as test_df was not loaded.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### **Run Judging, Analyze, and Summarize Results**\n",
        "Runs the judging process on generated answers, merges with ground truth, and computes final performance metrics for the combined guardrail experiment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 367
        },
        "id": "dBHYCOnl3ED6",
        "outputId": "f8b9262a-8e71-49b1-f02f-0a13aa0e7bca"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading datasets for judging and analysis...\n",
            "Guarded results: 617 prompts\n",
            "Baseline results: 617 prompts\n",
            "Loading secrets...\n",
            "Secrets loaded successfully.\n",
            "\n",
            "Starting judging process for combined guardrail results...\n",
            "\n",
            "--- Starting Corrected Judging Process for combined_guarded_judged_results.csv ---\n",
            "Initialized CSV file at: /home/ubuntu/HallucinationVectorProject/results/llama-3.1-8b/combined_guarded_judged_results.csv\n",
            "Found 0 already judged prompts. Resuming...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Judging combined_guarded_judged_results.csv:   0%|          | 0/617 [00:00<?, ?it/s]"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Judging combined_guarded_judged_results.csv:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 454/617 [42:41<15:19,  5.64s/it]  \n",
            "\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[11], line 36\u001b[0m\n\u001b[1;32m     33\u001b[0m start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 36\u001b[0m     \u001b[43mrun_judging_process\u001b[49m\u001b[43m(\u001b[49m\u001b[43mguarded_merged_df\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mGUARDED_JUDGED_PATH_COMBINED\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msecrets\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mSCALEDOWN_API_KEY\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     37\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mâœ“ Judging complete in \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m(time\u001b[38;5;241m.\u001b[39mtime()\u001b[38;5;250m \u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;250m \u001b[39mstart_time)\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m60\u001b[39m\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m minutes\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     38\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
            "File \u001b[0;32m~/HallucinationVectorProject/evaluate_guardrail.py:121\u001b[0m, in \u001b[0;36mrun_judging_process\u001b[0;34m(input_df, output_path, api_key)\u001b[0m\n\u001b[1;32m    119\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    120\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m3\u001b[39m): \u001b[38;5;66;03m# Retry logic\u001b[39;00m\n\u001b[0;32m--> 121\u001b[0m         score \u001b[38;5;241m=\u001b[39m \u001b[43mutils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_hallucination_score_0_100\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    122\u001b[0m \u001b[43m            \u001b[49m\u001b[43mapi_key\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mapi_key\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    123\u001b[0m \u001b[43m            \u001b[49m\u001b[43mquestion\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrow\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mQuestion\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Use the 'Question' column\u001b[39;49;00m\n\u001b[1;32m    124\u001b[0m \u001b[43m            \u001b[49m\u001b[43manswer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrow\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43manswer\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m          \u001b[49m\u001b[38;5;66;43;03m# Use the 'answer' column\u001b[39;49;00m\n\u001b[1;32m    125\u001b[0m \u001b[43m            \u001b[49m\u001b[43mbest_answer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrow\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mBest Answer\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Use the 'Best Answer' column\u001b[39;49;00m\n\u001b[1;32m    126\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcorrect_answers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrow\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mCorrect Answers\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    127\u001b[0m \u001b[43m            \u001b[49m\u001b[43mincorrect_answers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrow\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mIncorrect Answers\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[1;32m    128\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    129\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m score \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m    130\u001b[0m             \u001b[38;5;28;01mbreak\u001b[39;00m\n",
            "File \u001b[0;32m~/HallucinationVectorProject/utils.py:204\u001b[0m, in \u001b[0;36mget_hallucination_score_0_100\u001b[0;34m(api_key, question, answer, best_answer, correct_answers, incorrect_answers)\u001b[0m\n\u001b[1;32m    201\u001b[0m headers \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mx-api-key\u001b[39m\u001b[38;5;124m'\u001b[39m: api_key, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mContent-Type\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mapplication/json\u001b[39m\u001b[38;5;124m'\u001b[39m}\n\u001b[1;32m    203\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 204\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mrequests\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpost\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpayload\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m60\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    205\u001b[0m     response\u001b[38;5;241m.\u001b[39mraise_for_status()\n\u001b[1;32m    206\u001b[0m     content \u001b[38;5;241m=\u001b[39m response\u001b[38;5;241m.\u001b[39mjson()\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfull_response\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
            "File \u001b[0;32m~/HallucinationVectorProject/venv/lib/python3.10/site-packages/requests/api.py:115\u001b[0m, in \u001b[0;36mpost\u001b[0;34m(url, data, json, **kwargs)\u001b[0m\n\u001b[1;32m    103\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mpost\u001b[39m(url, data\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, json\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    104\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Sends a POST request.\u001b[39;00m\n\u001b[1;32m    105\u001b[0m \n\u001b[1;32m    106\u001b[0m \u001b[38;5;124;03m    :param url: URL for the new :class:`Request` object.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;124;03m    :rtype: requests.Response\u001b[39;00m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 115\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpost\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjson\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mjson\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/HallucinationVectorProject/venv/lib/python3.10/site-packages/requests/api.py:59\u001b[0m, in \u001b[0;36mrequest\u001b[0;34m(method, url, **kwargs)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;66;03m# By using the 'with' statement we are sure the session is closed, thus we\u001b[39;00m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;66;03m# avoid leaving sockets open which can trigger a ResourceWarning in some\u001b[39;00m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;66;03m# cases, and look like a memory leak in others.\u001b[39;00m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m sessions\u001b[38;5;241m.\u001b[39mSession() \u001b[38;5;28;01mas\u001b[39;00m session:\n\u001b[0;32m---> 59\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43msession\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/HallucinationVectorProject/venv/lib/python3.10/site-packages/requests/sessions.py:589\u001b[0m, in \u001b[0;36mSession.request\u001b[0;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[1;32m    584\u001b[0m send_kwargs \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    585\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtimeout\u001b[39m\u001b[38;5;124m\"\u001b[39m: timeout,\n\u001b[1;32m    586\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallow_redirects\u001b[39m\u001b[38;5;124m\"\u001b[39m: allow_redirects,\n\u001b[1;32m    587\u001b[0m }\n\u001b[1;32m    588\u001b[0m send_kwargs\u001b[38;5;241m.\u001b[39mupdate(settings)\n\u001b[0;32m--> 589\u001b[0m resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43msend_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    591\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m resp\n",
            "File \u001b[0;32m~/HallucinationVectorProject/venv/lib/python3.10/site-packages/requests/sessions.py:703\u001b[0m, in \u001b[0;36mSession.send\u001b[0;34m(self, request, **kwargs)\u001b[0m\n\u001b[1;32m    700\u001b[0m start \u001b[38;5;241m=\u001b[39m preferred_clock()\n\u001b[1;32m    702\u001b[0m \u001b[38;5;66;03m# Send the request\u001b[39;00m\n\u001b[0;32m--> 703\u001b[0m r \u001b[38;5;241m=\u001b[39m \u001b[43madapter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    705\u001b[0m \u001b[38;5;66;03m# Total elapsed time of the request (approximately)\u001b[39;00m\n\u001b[1;32m    706\u001b[0m elapsed \u001b[38;5;241m=\u001b[39m preferred_clock() \u001b[38;5;241m-\u001b[39m start\n",
            "File \u001b[0;32m~/HallucinationVectorProject/venv/lib/python3.10/site-packages/requests/adapters.py:644\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    641\u001b[0m     timeout \u001b[38;5;241m=\u001b[39m TimeoutSauce(connect\u001b[38;5;241m=\u001b[39mtimeout, read\u001b[38;5;241m=\u001b[39mtimeout)\n\u001b[1;32m    643\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 644\u001b[0m     resp \u001b[38;5;241m=\u001b[39m \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43murlopen\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    645\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    646\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    647\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    648\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    649\u001b[0m \u001b[43m        \u001b[49m\u001b[43mredirect\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    650\u001b[0m \u001b[43m        \u001b[49m\u001b[43massert_same_host\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    651\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    652\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    653\u001b[0m \u001b[43m        \u001b[49m\u001b[43mretries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    654\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    655\u001b[0m \u001b[43m        \u001b[49m\u001b[43mchunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    656\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    658\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (ProtocolError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m    659\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m(err, request\u001b[38;5;241m=\u001b[39mrequest)\n",
            "File \u001b[0;32m~/HallucinationVectorProject/venv/lib/python3.10/site-packages/urllib3/connectionpool.py:787\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[0m\n\u001b[1;32m    784\u001b[0m response_conn \u001b[38;5;241m=\u001b[39m conn \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m release_conn \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    786\u001b[0m \u001b[38;5;66;03m# Make the request on the HTTPConnection object\u001b[39;00m\n\u001b[0;32m--> 787\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    788\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    789\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    790\u001b[0m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    791\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout_obj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    792\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    793\u001b[0m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    794\u001b[0m \u001b[43m    \u001b[49m\u001b[43mchunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    795\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    796\u001b[0m \u001b[43m    \u001b[49m\u001b[43mresponse_conn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresponse_conn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    797\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpreload_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    798\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecode_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    799\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mresponse_kw\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    800\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    802\u001b[0m \u001b[38;5;66;03m# Everything went great!\u001b[39;00m\n\u001b[1;32m    803\u001b[0m clean_exit \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
            "File \u001b[0;32m~/HallucinationVectorProject/venv/lib/python3.10/site-packages/urllib3/connectionpool.py:534\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[0;34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[0m\n\u001b[1;32m    532\u001b[0m \u001b[38;5;66;03m# Receive the response from the server\u001b[39;00m\n\u001b[1;32m    533\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 534\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetresponse\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    535\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (BaseSSLError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    536\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_raise_timeout(err\u001b[38;5;241m=\u001b[39me, url\u001b[38;5;241m=\u001b[39murl, timeout_value\u001b[38;5;241m=\u001b[39mread_timeout)\n",
            "File \u001b[0;32m~/HallucinationVectorProject/venv/lib/python3.10/site-packages/urllib3/connection.py:565\u001b[0m, in \u001b[0;36mHTTPConnection.getresponse\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    562\u001b[0m _shutdown \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msock, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mshutdown\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    564\u001b[0m \u001b[38;5;66;03m# Get the response from http.client.HTTPConnection\u001b[39;00m\n\u001b[0;32m--> 565\u001b[0m httplib_response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetresponse\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    567\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    568\u001b[0m     assert_header_parsing(httplib_response\u001b[38;5;241m.\u001b[39mmsg)\n",
            "File \u001b[0;32m/usr/lib/python3.10/http/client.py:1375\u001b[0m, in \u001b[0;36mHTTPConnection.getresponse\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1373\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1374\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1375\u001b[0m         \u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbegin\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1376\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m:\n\u001b[1;32m   1377\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclose()\n",
            "File \u001b[0;32m/usr/lib/python3.10/http/client.py:318\u001b[0m, in \u001b[0;36mHTTPResponse.begin\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    316\u001b[0m \u001b[38;5;66;03m# read until we get a non-100 response\u001b[39;00m\n\u001b[1;32m    317\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 318\u001b[0m     version, status, reason \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_read_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    319\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m status \u001b[38;5;241m!=\u001b[39m CONTINUE:\n\u001b[1;32m    320\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
            "File \u001b[0;32m/usr/lib/python3.10/http/client.py:279\u001b[0m, in \u001b[0;36mHTTPResponse._read_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    278\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_read_status\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m--> 279\u001b[0m     line \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreadline\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_MAXLINE\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124miso-8859-1\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    280\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(line) \u001b[38;5;241m>\u001b[39m _MAXLINE:\n\u001b[1;32m    281\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m LineTooLong(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstatus line\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
            "File \u001b[0;32m/usr/lib/python3.10/socket.py:705\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    703\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m    704\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 705\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    706\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[1;32m    707\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout_occurred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
            "File \u001b[0;32m/usr/lib/python3.10/ssl.py:1303\u001b[0m, in \u001b[0;36mSSLSocket.recv_into\u001b[0;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[1;32m   1299\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m flags \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m   1300\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1301\u001b[0m           \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnon-zero flags not allowed in calls to recv_into() on \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m\n\u001b[1;32m   1302\u001b[0m           \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m)\n\u001b[0;32m-> 1303\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnbytes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1304\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1305\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mrecv_into(buffer, nbytes, flags)\n",
            "File \u001b[0;32m/usr/lib/python3.10/ssl.py:1159\u001b[0m, in \u001b[0;36mSSLSocket.read\u001b[0;34m(self, len, buffer)\u001b[0m\n\u001b[1;32m   1157\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1158\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m buffer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1159\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sslobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1160\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1161\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sslobj\u001b[38;5;241m.\u001b[39mread(\u001b[38;5;28mlen\u001b[39m)\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "from evaluate_guardrail import run_judging_process\n",
        "import utils\n",
        "import config\n",
        "import pandas as pd\n",
        "import time\n",
        "\n",
        "# --- Redefine paths for the analysis (using local paths) ---\n",
        "GUARDED_JUDGED_PATH_COMBINED = RESULTS_DIR / \"combined_guarded_judged_results.csv\"\n",
        "BASELINE_JUDGED_RESULTS_PATH = RESULTS_DIR / \"baseline_judged_results_truthfulqa.csv\"\n",
        "GUARDED_RESULTS_PATH_COMBINED = RESULTS_DIR / \"combined_guarded_results.csv\"\n",
        "BASELINE_RESULTS_PATH = os.path.join(PROJECT_DIR, \"baseline_results_truthfulqa.csv\")\n",
        "\n",
        "print(\"Loading datasets for judging and analysis...\")\n",
        "\n",
        "# Load the test set\n",
        "test_df = pd.read_csv(DATA_DIR / \"final_test_set_truthfulqa.csv\")\n",
        "\n",
        "# Load the newly generated results\n",
        "guarded_df = pd.read_csv(GUARDED_RESULTS_PATH_COMBINED)\n",
        "baseline_df = pd.read_csv(BASELINE_RESULTS_PATH)\n",
        "\n",
        "# Merge with ground truth\n",
        "guarded_merged_df = pd.merge(guarded_df, test_df, left_on='prompt', right_on='Question', how='left')\n",
        "baseline_merged_df = pd.merge(baseline_df, test_df, left_on='prompt', right_on='Question', how='left')\n",
        "\n",
        "print(f\"Guarded results: {len(guarded_merged_df)} prompts\")\n",
        "print(f\"Baseline results: {len(baseline_merged_df)} prompts\")\n",
        "\n",
        "# --- Run Judging with retry logic for network stability ---\n",
        "secrets = utils.load_secrets()\n",
        "\n",
        "print(\"\\nStarting judging process for combined guardrail results...\")\n",
        "start_time = time.time()\n",
        "\n",
        "try:\n",
        "    run_judging_process(guarded_merged_df, GUARDED_JUDGED_PATH_COMBINED, secrets['SCALEDOWN_API_KEY'])\n",
        "    print(f\"âœ“ Judging complete in {(time.time() - start_time)/60:.2f} minutes\")\n",
        "except Exception as e:\n",
        "    print(f\"Error during judging: {e}\")\n",
        "\n",
        "# Assuming baseline is already judged, if not, uncomment below\n",
        "run_judging_process(baseline_merged_df, BASELINE_JUDGED_RESULTS_PATH, secrets['SCALEDOWN_API_KEY'])\n",
        "\n",
        "# --- Analyze and Print Final Report ---\n",
        "print(\"\\nAnalyzing final performance metrics...\")\n",
        "\n",
        "guarded_judged_df = pd.read_csv(GUARDED_JUDGED_PATH_COMBINED)\n",
        "baseline_judged_df = pd.read_csv(BASELINE_JUDGED_RESULTS_PATH)\n",
        "\n",
        "baseline_accuracy = baseline_judged_df['is_correct'].mean()\n",
        "guarded_accuracy = guarded_judged_df['is_correct'].mean()\n",
        "baseline_error_rate = 1 - baseline_accuracy\n",
        "guarded_error_rate = 1 - guarded_accuracy\n",
        "relative_error_reduction = (baseline_error_rate - guarded_error_rate) / baseline_error_rate if baseline_error_rate > 0 else 0\n",
        "baseline_latency = baseline_judged_df['latency_seconds'].mean()\n",
        "guarded_latency = guarded_judged_df['latency_seconds'].mean()\n",
        "latency_increase_percent = (guarded_latency - baseline_latency) / baseline_latency * 100\n",
        "\n",
        "summary_data = {\n",
        "    \"Metric\": [\"Accuracy\", \"Hallucination Rate\", \"Avg Latency (s)\", \"Relative Error Reduction\", \"Latency Increase\"],\n",
        "    \"Baseline Model\": [f\"{baseline_accuracy:.2%}\", f\"{baseline_error_rate:.2%}\", f\"{baseline_latency:.2f}\", \"N/A\", \"N/A\"],\n",
        "    \"Guarded Model (Combined)\": [f\"{guarded_accuracy:.2%}\", f\"{guarded_error_rate:.2%}\", f\"{guarded_latency:.2f}\", f\"{relative_error_reduction:.2%}\", f\"{latency_increase_percent:+.2f}%\"],\n",
        "}\n",
        "summary_df = pd.DataFrame(summary_data)\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"FINAL PERFORMANCE SUMMARY (Combined Dynamic Alpha + Selective N-Tokens)\")\n",
        "print(\"=\"*80)\n",
        "print(summary_df.to_string(index=False))\n",
        "print(\"=\"*80)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "0a130b649922465fbaaa39360bc182b3": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0ff04e29983d44f4b0500bd187413494",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_fb5f686ae0d846daa92a3f7eca0cb09c",
            "value": 1
          }
        },
        "0af727690a8c45189de6b891f5544520": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0ed0d6557757456b9e17a68f962a6101": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0ff04e29983d44f4b0500bd187413494": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "12bd09bfc4d64f2897444365ae8fcf55": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "16c66f714ed4455d98d2d908aac474e9": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0ed0d6557757456b9e17a68f962a6101",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_7085b78429354064a9fab9ce02859ab1",
            "value": "â€‡345/345â€‡[00:00&lt;00:00,â€‡24.5kB/s]"
          }
        },
        "1ad470b0a62d4601a1519bae23982985": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1cc59559f4714bb68164d44fbe9c4388": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_58e1ff15b9af4bab8225b462de199ef1",
              "IPY_MODEL_d9ebdf759a5347609c1ae2f30d7a73a1",
              "IPY_MODEL_16c66f714ed4455d98d2d908aac474e9"
            ],
            "layout": "IPY_MODEL_659934e23afd4d2ba94f672058a226c8"
          }
        },
        "1ce0fe58bdf04419bba04b24bb6b3d44": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_390a34c1d7c44413888da54078fed4ee",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_f757bbf5954e4ad093e50081ce4d4187",
            "value": "â€‡51.1k/?â€‡[00:00&lt;00:00,â€‡5.25MB/s]"
          }
        },
        "21b5555a566947a395837924a0425748": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "27cd1ec6893242ceade77c811d0f4c20": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2a8f6933fd824103be3ae29cfb4aff07": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "2b09d3f2094d4906a56883d3f467bcba": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_cc268fe08c5649c6a172c887ea637b8e",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_baffd1a874b84e69bf0dabefffa6fa5c",
            "value": "â€‡220/220â€‡[00:00&lt;00:00,â€‡22.8kB/s]"
          }
        },
        "33b418ea3b974d788e1b0e736ae59414": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "390a34c1d7c44413888da54078fed4ee": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "39c05fc5cece43c696cb21be0d7598fb": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0af727690a8c45189de6b891f5544520",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_9ef056edfefb42859c1017479e729f8a",
            "value": "tokenizer_config.json:â€‡"
          }
        },
        "4f5b35cbc6774df3b1fe397b14640933": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5848feacc80a4506acd04c7ab328720c": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "58e1ff15b9af4bab8225b462de199ef1": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_92f6b3ced073463089b038f2bc506e87",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_f104dba960124583a9e01cdedb77fec2",
            "value": "special_tokens_map.json:â€‡100%"
          }
        },
        "5937ff2ff09145a1aed178187eec6b11": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5848feacc80a4506acd04c7ab328720c",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_6af94307d3504f029600735742ec2ba8",
            "value": "â€‡5.70G/5.70Gâ€‡[00:40&lt;00:00,â€‡197MB/s]"
          }
        },
        "59ec59c5eb22439989621785442232a3": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "5a8876aa9160455aaace1cff34d3053d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_853eae85943e4f059bb4f284ab03857a",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_27cd1ec6893242ceade77c811d0f4c20",
            "value": "â€‡9.09M/?â€‡[00:00&lt;00:00,â€‡88.8MB/s]"
          }
        },
        "659934e23afd4d2ba94f672058a226c8": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6af94307d3504f029600735742ec2ba8": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7085b78429354064a9fab9ce02859ab1": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "79cbba6bd54549bd8131cd887d2e0708": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "79ee4dd2310a4113b30d00dd7b9f42e9": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "853eae85943e4f059bb4f284ab03857a": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "85efeb56ef7b4ec2b69753e2d8d16abb": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "8b98fbbe2d51452aa3605cc56fd45703": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_33b418ea3b974d788e1b0e736ae59414",
            "max": 5702746403,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_85efeb56ef7b4ec2b69753e2d8d16abb",
            "value": 5702746403
          }
        },
        "8bfec65739ab4b2f8c11a995172f3623": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "905c490e14704d99a8050dcdfea8a4aa": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "92f6b3ced073463089b038f2bc506e87": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "93dc8d4505ff405585edcf873831fccb": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_21b5555a566947a395837924a0425748",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_ce416e14219d473aa127531c4e806b0f",
            "value": "tokenizer.json:â€‡"
          }
        },
        "9ef056edfefb42859c1017479e729f8a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a2929e498fd2482681c9ab3dd7a18856": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a4a1ed5428f14053b589bd5760905c23",
            "max": 220,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_eb9d8be9ae874961b365535a32d3bdfa",
            "value": 220
          }
        },
        "a4a1ed5428f14053b589bd5760905c23": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b0f86b759fa34618840fd7f6426ca31b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_b2c0965a7c8f4e06841bc97baf6913cc",
              "IPY_MODEL_8b98fbbe2d51452aa3605cc56fd45703",
              "IPY_MODEL_5937ff2ff09145a1aed178187eec6b11"
            ],
            "layout": "IPY_MODEL_1ad470b0a62d4601a1519bae23982985"
          }
        },
        "b1caba83cb704bd98a96128eece691e0": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_93dc8d4505ff405585edcf873831fccb",
              "IPY_MODEL_bb2b2b8149da46c2909a9de140c1be37",
              "IPY_MODEL_5a8876aa9160455aaace1cff34d3053d"
            ],
            "layout": "IPY_MODEL_e2f0f384cb344d49af8c1eb063f0c370"
          }
        },
        "b2bf26fd9bef4f3da213f856ee74260c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_de91c5159db443b0bf0caf88e72d312b",
              "IPY_MODEL_a2929e498fd2482681c9ab3dd7a18856",
              "IPY_MODEL_2b09d3f2094d4906a56883d3f467bcba"
            ],
            "layout": "IPY_MODEL_79cbba6bd54549bd8131cd887d2e0708"
          }
        },
        "b2c0965a7c8f4e06841bc97baf6913cc": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8bfec65739ab4b2f8c11a995172f3623",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_4f5b35cbc6774df3b1fe397b14640933",
            "value": "model.safetensors:â€‡100%"
          }
        },
        "baffd1a874b84e69bf0dabefffa6fa5c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "bb2b2b8149da46c2909a9de140c1be37": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_79ee4dd2310a4113b30d00dd7b9f42e9",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_2a8f6933fd824103be3ae29cfb4aff07",
            "value": 1
          }
        },
        "cc268fe08c5649c6a172c887ea637b8e": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ccec744d07344082beaad09c8e756b59": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ce416e14219d473aa127531c4e806b0f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d9ebdf759a5347609c1ae2f30d7a73a1": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ee561b9b6117400b80def264d5cc0c5a",
            "max": 345,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_59ec59c5eb22439989621785442232a3",
            "value": 345
          }
        },
        "de91c5159db443b0bf0caf88e72d312b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_12bd09bfc4d64f2897444365ae8fcf55",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_905c490e14704d99a8050dcdfea8a4aa",
            "value": "generation_config.json:â€‡100%"
          }
        },
        "e2f0f384cb344d49af8c1eb063f0c370": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "eb9d8be9ae874961b365535a32d3bdfa": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "ee561b9b6117400b80def264d5cc0c5a": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f104dba960124583a9e01cdedb77fec2": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f757bbf5954e4ad093e50081ce4d4187": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "fb5f686ae0d846daa92a3f7eca0cb09c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "fb9418c321b74ca2905264a7015a3cf7": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_39c05fc5cece43c696cb21be0d7598fb",
              "IPY_MODEL_0a130b649922465fbaaa39360bc182b3",
              "IPY_MODEL_1ce0fe58bdf04419bba04b24bb6b3d44"
            ],
            "layout": "IPY_MODEL_ccec744d07344082beaad09c8e756b59"
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
