{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Combined Dynamic Alpha + Selective N-Tokens Steering: Hallucination Guardrail Evaluation\n",
        " \n",
        "**Summary:**\n",
        "This notebook evaluates a combined guardrail approach for Llama-3.1-8B that integrates both Dynamic Alpha (risk-proportional steering strength) and Selective N-Tokens Steering (applying the intervention only to the first N tokens). This method applies a dynamic, risk-scaled correction for high-risk prompts, but only during the initial generation steps, maximizing hallucination reduction while minimizing latency and preserving answer quality. This combined approach outperforms both individual ablations and is used for all further evaluations on other datasets.\n",
        "\n",
        "- **Dynamic Alpha:** Steering strength (alpha) is scaled based on prompt risk, providing stronger correction for riskier prompts.\n",
        "- **Selective N-Tokens:** Steering is applied only to the first 10 generated tokens, focusing intervention where it is most effective.\n",
        "\n",
        "**Key Results (TruthfulQA Benchmark):**\n",
        "- **Baseline Model:** Accuracy: 38.57%, Hallucination Rate: 61.43%, Avg Latency: 3.86s\n",
        "- **Combined Guarded Model:** Accuracy: 52.04%, Hallucination Rate: 47.96%, Avg Latency: 3.56s\n",
        "- **Relative Error Reduction:** 21.93%\n",
        "- **Latency Increase:** -7.78% (latency decreased)\n",
        "\n",
        "This combined guardrail achieves the best trade-off between hallucination reduction, accuracy, and latency, and is therefore used for all subsequent cross-domain evaluations."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### **Environment and Requirements Setup**\n",
        "Setup for local execution on Lambda Labs A100 40GB GPU with Llama-3.1-8B."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NFpTX3fPzY6W",
        "outputId": "571ad1d1-3c15-423e-95b6-97f15a632507"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Environment verification:\n",
            "Python version: 3.10.12 (main, May 27 2025, 17:12:29) [GCC 11.4.0]\n",
            "PyTorch version: 2.9.0+cu128\n",
            "CUDA available: True\n",
            "CUDA version: 12.8\n",
            "GPU count: 1\n",
            "  GPU 0: NVIDIA A100-SXM4-40GB\n"
          ]
        }
      ],
      "source": [
        "\n",
        "import os\n",
        "import sys\n",
        "# Verify environment\n",
        "import torch\n",
        "print(f\"\\nEnvironment verification:\")\n",
        "print(f\"Python version: {sys.version}\")\n",
        "print(f\"PyTorch version: {torch.__version__}\")\n",
        "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"CUDA version: {torch.version.cuda}\")\n",
        "    print(f\"GPU count: {torch.cuda.device_count()}\")\n",
        "    for i in range(torch.cuda.device_count()):\n",
        "        print(f\"  GPU {i}: {torch.cuda.get_device_name(i)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✓ API keys loaded successfully from environment variables\n",
            "  HF_TOKEN: hf_NrlndFS...\n",
            "  SCALEDOWN_API_KEY: OMJ5hWc0m4...\n"
          ]
        }
      ],
      "source": [
        "# Load API Keys from environment variables\n",
        "import os\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "load_dotenv()\n",
        "\n",
        "# Load the keys from environment variables\n",
        "try:\n",
        "    HF_TOKEN = os.environ.get('HF_TOKEN')\n",
        "    SCALEDOWN_API_KEY = os.environ.get('SCALEDOWN_API_KEY')\n",
        "    \n",
        "    if not HF_TOKEN:\n",
        "        raise ValueError(\"HF_TOKEN not found in environment variables. Please set it before running.\")\n",
        "    if not SCALEDOWN_API_KEY:\n",
        "        raise ValueError(\"SCALEDOWN_API_KEY not found in environment variables. Please set it before running.\")\n",
        "    \n",
        "    # Set HuggingFace token in environment for model loading\n",
        "    os.environ[\"HF_TOKEN\"] = HF_TOKEN\n",
        "    \n",
        "    print(\"✓ API keys loaded successfully from environment variables\")\n",
        "    print(f\"  HF_TOKEN: {HF_TOKEN[:10]}...\" if HF_TOKEN else \"  HF_TOKEN: Not set\")\n",
        "    print(f\"  SCALEDOWN_API_KEY: {SCALEDOWN_API_KEY[:10]}...\" if SCALEDOWN_API_KEY else \"  SCALEDOWN_API_KEY: Not set\")\n",
        "    \n",
        "except Exception as e:\n",
        "    print(f\"ERROR loading API keys: {e}\")\n",
        "    print(\"Please ensure you have set the following environment variables:\")\n",
        "    print(\"  export HF_TOKEN='your_huggingface_token'\")\n",
        "    print(\"  export SCALEDOWN_API_KEY='your_scaledown_api_key'\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### **Project Path Setup**\n",
        "Sets up local project paths for Lambda Labs execution."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lr7jpBx7z79b",
        "outputId": "aeca7557-5812-42ca-816a-6a6c21cd9a5d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Project directory: /home/ubuntu/HallucinationVectorProject\n",
            "Data directory: /home/ubuntu/HallucinationVectorProject/data\n",
            "Artifacts directory: /home/ubuntu/HallucinationVectorProject/artifacts/llama-3.1-8b\n",
            "Results directory: /home/ubuntu/HallucinationVectorProject/results/llama-3.1-8b\n",
            "✓ Environment configured for local Lambda Labs execution.\n"
          ]
        }
      ],
      "source": [
        "import sys\n",
        "import os\n",
        "from pathlib import Path\n",
        "\n",
        "# Setup local project paths\n",
        "PROJECT_DIR = Path(\"/home/ubuntu/HallucinationVectorProject\")\n",
        "DATA_DIR = PROJECT_DIR / \"data\"\n",
        "ARTIFACTS_DIR = PROJECT_DIR / \"artifacts\" / \"llama-3.1-8b\"\n",
        "RESULTS_DIR = PROJECT_DIR / \"results\" / \"llama-3.1-8b\"\n",
        "\n",
        "# Create directories if they don't exist\n",
        "ARTIFACTS_DIR.mkdir(parents=True, exist_ok=True)\n",
        "RESULTS_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# Add project directory to Python's path\n",
        "project_path = str(PROJECT_DIR)\n",
        "if project_path not in sys.path:\n",
        "    sys.path.append(project_path)\n",
        "\n",
        "print(f\"Project directory: {PROJECT_DIR}\")\n",
        "print(f\"Data directory: {DATA_DIR}\")\n",
        "print(f\"Artifacts directory: {ARTIFACTS_DIR}\")\n",
        "print(f\"Results directory: {RESULTS_DIR}\")\n",
        "\n",
        "# Programmatically set the environment to 'local' in the config file\n",
        "config_file_path = PROJECT_DIR / 'config.py'\n",
        "with open(config_file_path, 'r') as f:\n",
        "    lines = f.readlines()\n",
        "with open(config_file_path, 'w') as f:\n",
        "    for line in lines:\n",
        "        if line.strip().startswith('ENVIRONMENT ='):\n",
        "            f.write('ENVIRONMENT = \"local\"\\n')\n",
        "        else:\n",
        "            f.write(line)\n",
        "print(\"✓ Environment configured for local Lambda Labs execution.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### **Load Artifacts and Model Setup**\n",
        "Loads all required model artifacts for Llama-3.1-8B, including the hallucination vector, risk classifier, and config thresholds, and prepares the model for evaluation on A100 40GB GPU."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 373,
          "referenced_widgets": [
            "b0f86b759fa34618840fd7f6426ca31b",
            "b2c0965a7c8f4e06841bc97baf6913cc",
            "8b98fbbe2d51452aa3605cc56fd45703",
            "5937ff2ff09145a1aed178187eec6b11",
            "1ad470b0a62d4601a1519bae23982985",
            "8bfec65739ab4b2f8c11a995172f3623",
            "4f5b35cbc6774df3b1fe397b14640933",
            "33b418ea3b974d788e1b0e736ae59414",
            "85efeb56ef7b4ec2b69753e2d8d16abb",
            "5848feacc80a4506acd04c7ab328720c",
            "6af94307d3504f029600735742ec2ba8",
            "b2bf26fd9bef4f3da213f856ee74260c",
            "de91c5159db443b0bf0caf88e72d312b",
            "a2929e498fd2482681c9ab3dd7a18856",
            "2b09d3f2094d4906a56883d3f467bcba",
            "79cbba6bd54549bd8131cd887d2e0708",
            "12bd09bfc4d64f2897444365ae8fcf55",
            "905c490e14704d99a8050dcdfea8a4aa",
            "a4a1ed5428f14053b589bd5760905c23",
            "eb9d8be9ae874961b365535a32d3bdfa",
            "cc268fe08c5649c6a172c887ea637b8e",
            "baffd1a874b84e69bf0dabefffa6fa5c",
            "fb9418c321b74ca2905264a7015a3cf7",
            "39c05fc5cece43c696cb21be0d7598fb",
            "0a130b649922465fbaaa39360bc182b3",
            "1ce0fe58bdf04419bba04b24bb6b3d44",
            "ccec744d07344082beaad09c8e756b59",
            "0af727690a8c45189de6b891f5544520",
            "9ef056edfefb42859c1017479e729f8a",
            "0ff04e29983d44f4b0500bd187413494",
            "fb5f686ae0d846daa92a3f7eca0cb09c",
            "390a34c1d7c44413888da54078fed4ee",
            "f757bbf5954e4ad093e50081ce4d4187",
            "b1caba83cb704bd98a96128eece691e0",
            "93dc8d4505ff405585edcf873831fccb",
            "bb2b2b8149da46c2909a9de140c1be37",
            "5a8876aa9160455aaace1cff34d3053d",
            "e2f0f384cb344d49af8c1eb063f0c370",
            "21b5555a566947a395837924a0425748",
            "ce416e14219d473aa127531c4e806b0f",
            "79ee4dd2310a4113b30d00dd7b9f42e9",
            "2a8f6933fd824103be3ae29cfb4aff07",
            "853eae85943e4f059bb4f284ab03857a",
            "27cd1ec6893242ceade77c811d0f4c20",
            "1cc59559f4714bb68164d44fbe9c4388",
            "58e1ff15b9af4bab8225b462de199ef1",
            "d9ebdf759a5347609c1ae2f30d7a73a1",
            "16c66f714ed4455d98d2d908aac474e9",
            "659934e23afd4d2ba94f672058a226c8",
            "92f6b3ced073463089b038f2bc506e87",
            "f104dba960124583a9e01cdedb77fec2",
            "ee561b9b6117400b80def264d5cc0c5a",
            "59ec59c5eb22439989621785442232a3",
            "0ed0d6557757456b9e17a68f962a6101",
            "7085b78429354064a9fab9ce02859ab1"
          ]
        },
        "id": "X7eAPnlB0VVQ",
        "outputId": "235bd42a-a834-4e6b-84ff-4fd49bf52193"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "🦥 Unsloth: Will patch your computer to enable 2x faster free finetuning.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[xformers|WARNING]WARNING[XFORMERS]: xFormers can't load C++/CUDA extensions. xFormers was built for:\n",
            "    PyTorch 2.9.0+cu130 with CUDA 1300 (you have 2.9.0+cu128)\n",
            "    Python  3.12.12 (you have 3.10.12)\n",
            "  Please reinstall xformers (see https://github.com/facebookresearch/xformers#installing-xformers)\n",
            "  Memory-efficient attention, SwiGLU, sparse and more won't be available.\n",
            "  Set XFORMERS_MORE_DETAILS=1 for more details\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "========\n",
            "Switching to PyTorch attention since your Xformers is broken.\n",
            "========\n",
            "\n",
            "Unsloth: Xformers was not installed correctly.\n",
            "Please install xformers separately first.\n",
            "Then confirm if it's correctly installed by running:\n",
            "python -m xformers.info\n",
            "\n",
            "Longer error message:\n",
            "xFormers can't load C++/CUDA extensions. xFormers was built for:\n",
            "    PyTorch 2.9.0+cu130 with CUDA 1300 (you have 2.9.0+cu128)\n",
            "    Python  3.12.12 (you have 3.10.12)\n",
            "  Please reinstall xformers (see https://github.com/facebookresearch/xformers#installing-xformers)\n",
            "  Memory-efficient attention, SwiGLU, sparse and more won't be available.\n",
            "🦥 Unsloth Zoo will now patch everything to make training faster!\n"
          ]
        },
        {
          "ename": "KeyError",
          "evalue": "'align_logprobs_with_mask'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[3], line 9\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtqdm\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m tqdm\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mjoblib\u001b[39;00m\n\u001b[0;32m----> 9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01munsloth\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m FastLanguageModel\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# Ensure project directory is in path before importing custom modules\u001b[39;00m\n\u001b[1;32m     12\u001b[0m PROJECT_DIR \u001b[38;5;241m=\u001b[39m Path(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/home/ubuntu/HallucinationVectorProject\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
            "File \u001b[0;32m~/HallucinationVectorProject/venv/lib/python3.10/site-packages/unsloth/__init__.py:211\u001b[0m\n\u001b[1;32m    208\u001b[0m     \u001b[38;5;66;03m# TODO: check triton for intel installed properly.\u001b[39;00m\n\u001b[1;32m    209\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[0;32m--> 211\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[1;32m    212\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m __version__\n\u001b[1;32m    213\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msave\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n",
            "File \u001b[0;32m~/HallucinationVectorProject/venv/lib/python3.10/site-packages/unsloth/models/__init__.py:15\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Copyright 2023-present Daniel Han-Chen & the Unsloth team. All rights reserved.\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Licensed under the Apache License, Version 2.0 (the \"License\");\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# See the License for the specific language governing permissions and\u001b[39;00m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# limitations under the License.\u001b[39;00m\n\u001b[0;32m---> 15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mllama\u001b[39;00m\u001b[38;5;250m     \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m FastLlamaModel\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mloader\u001b[39;00m\u001b[38;5;250m    \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m FastLanguageModel, FastVisionModel, FastTextModel, FastModel\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmistral\u001b[39;00m\u001b[38;5;250m   \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m FastMistralModel\n",
            "File \u001b[0;32m~/HallucinationVectorProject/venv/lib/python3.10/site-packages/unsloth/models/llama.py:3050\u001b[0m\n\u001b[1;32m   3047\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[1;32m   3048\u001b[0m \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[0;32m-> 3050\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mrl\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m PatchFastRL\n\u001b[1;32m   3051\u001b[0m PatchFastRL(FastLanguageModel \u001b[38;5;241m=\u001b[39m FastLlamaModel)\n",
            "File \u001b[0;32m~/HallucinationVectorProject/venv/lib/python3.10/site-packages/unsloth/models/rl.py:186\u001b[0m\n\u001b[1;32m    184\u001b[0m create_completion_attention_mask \u001b[38;5;241m=\u001b[39m RL_REPLACEMENTS[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcreate_completion_attention_mask\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    185\u001b[0m left_pack_padding                \u001b[38;5;241m=\u001b[39m RL_REPLACEMENTS[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mleft_pack_padding\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m--> 186\u001b[0m align_logprobs_with_mask         \u001b[38;5;241m=\u001b[39m \u001b[43mRL_REPLACEMENTS\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43malign_logprobs_with_mask\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[1;32m    188\u001b[0m RLTrainer_replacement \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'''\u001b[39m\n\u001b[1;32m    189\u001b[0m \u001b[38;5;124mimport os\u001b[39m\n\u001b[1;32m    190\u001b[0m \u001b[38;5;124mfrom typing import *\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    282\u001b[0m \u001b[38;5;124mpass\u001b[39m\n\u001b[1;32m    283\u001b[0m \u001b[38;5;124m'''\u001b[39m\n\u001b[1;32m    285\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_patch_trl_rl_trainers\u001b[39m(trainer_file \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgrpo_trainer\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    286\u001b[0m     \u001b[38;5;66;03m# Patch for vLLM and Unsloth PEFT\u001b[39;00m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'align_logprobs_with_mask'"
          ]
        }
      ],
      "source": [
        "import time\n",
        "import pandas as pd\n",
        "import torch\n",
        "import csv\n",
        "import sys\n",
        "from pathlib import Path\n",
        "from tqdm import tqdm\n",
        "import joblib\n",
        "from unsloth import FastLanguageModel\n",
        "\n",
        "# Ensure project directory is in path before importing custom modules\n",
        "PROJECT_DIR = Path(\"/home/ubuntu/HallucinationVectorProject\")\n",
        "if str(PROJECT_DIR) not in sys.path:\n",
        "    sys.path.insert(0, str(PROJECT_DIR))\n",
        "\n",
        "# Import custom modules\n",
        "\n",
        "\n",
        "os.environ[\"UNSLOTH_STABLE_DOWNLOADS\"] = \"1\"\n",
        "\n",
        "\n",
        "# Helper function to monitor GPU memory\n",
        "def print_gpu_memory():\n",
        "    \"\"\"Print memory usage for all available GPUs.\"\"\"\n",
        "    if torch.cuda.is_available():\n",
        "        allocated = torch.cuda.memory_allocated(0) / 1024**3\n",
        "        reserved = torch.cuda.memory_reserved(0) / 1024**3\n",
        "        total = torch.cuda.get_device_properties(0).total_memory / 1024**3\n",
        "        print(f\"GPU 0 ({torch.cuda.get_device_name(0)}): \"\n",
        "              f\"{allocated:.2f}GB allocated, {reserved:.2f}GB reserved, {total:.2f}GB total\")\n",
        "\n",
        "# This global dictionary will hold our models, tokenizer, vectors, etc.\n",
        "artifacts = {}\n",
        "\n",
        "def load_all_artifacts():\n",
        "    \"\"\"Loads all necessary model and project artifacts into the global dict.\"\"\"\n",
        "    if artifacts: return\n",
        "    print(\"Loading all necessary artifacts for Llama-3.1-8B evaluation...\")\n",
        "    \n",
        "    print(\"\\nGPU memory before model loading:\")\n",
        "    print_gpu_memory()\n",
        "    \n",
        "    # Clear any cached memory\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.empty_cache()\n",
        "    \n",
        "    # Load 8B model for single GPU\n",
        "    print(\"\\nLoading Llama-3.1-8B model (bfloat16)...\")\n",
        "    model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "        model_name=\"unsloth/Meta-Llama-3.1-8B-Instruct\",\n",
        "        max_seq_length=4096,\n",
        "        dtype=torch.bfloat16,\n",
        "        load_in_4bit=False,\n",
        "        trust_remote_code=True,\n",
        "    )\n",
        "\n",
        "    # Configure for inference\n",
        "    model = FastLanguageModel.for_inference(model)\n",
        "    model.gradient_checkpointing_disable()\n",
        "    model.config.gradient_checkpointing = False\n",
        "    model.config.use_cache = True\n",
        "    model.eval()\n",
        "\n",
        "    artifacts['model'] = model\n",
        "    artifacts['tokenizer'] = tokenizer\n",
        "    artifacts['v_halluc'] = torch.load(ARTIFACTS_DIR / \"v_halluc.pt\").to(model.device).to(torch.bfloat16)\n",
        "    artifacts['risk_classifier'] = joblib.load(ARTIFACTS_DIR / \"risk_clf.joblib\")\n",
        "    artifacts['thresholds'] = {\n",
        "        \"tau_low\": 0.0390,\n",
        "        \"tau_high\": 0.0547,\n",
        "        \"optimal_alpha\": -1.0\n",
        "    }\n",
        "    \n",
        "    print(\"\\n✓ All artifacts loaded successfully!\")\n",
        "    print(f\"Model device: {model.device}\")\n",
        "    print(\"\\nGPU memory after loading:\")\n",
        "    print_gpu_memory()\n",
        "\n",
        "# Load everything\n",
        "load_all_artifacts()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**FIXED:** If you encounter a `KeyError: 'align_logprobs_with_mask'` error when importing unsloth, this is due to version incompatibility between Unsloth and TRL library changes from recent hours.\n",
        "\n",
        "**Solution:**\n",
        "```bash\n",
        "# Uninstall conflicting packages\n",
        "pip uninstall -y unsloth unsloth-zoo trl transformers peft\n",
        "\n",
        "# Install compatible base versions\n",
        "pip install transformers==4.46.2 peft==0.13.2 trl==0.12.1\n",
        "\n",
        "# Install Unsloth without dependencies, then unsloth-zoo\n",
        "pip install --upgrade --no-cache-dir --no-deps \"unsloth @ git+https://github.com/unslothai/unsloth.git\"\n",
        "pip install unsloth-zoo\n",
        "```\n",
        "\n",
        "This installs:\n",
        "- **transformers**: 4.57.1 (auto-upgraded by unsloth-zoo)\n",
        "- **trl**: 0.23.0 (auto-upgraded by unsloth-zoo)  \n",
        "- **peft**: 0.13.2\n",
        "- **tokenizers**: 0.22.1\n",
        "- **unsloth**: 2025.10.11\n",
        "- **unsloth-zoo**: 2025.10.12\n",
        "\n",
        "**After reinstalling, RESTART THE KERNEL before running cells.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### **Combined Selective Activation Steering and Guardrail Function**\n",
        "Defines a context manager to apply the steering vector only to the first N tokens, with dynamic risk-proportional strength, and a function to generate answers using this combined intervention."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mKmDgmbl0hW5",
        "outputId": "f5a0c6f6-71ec-4e9b-c944-43b2683fef08"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/ubuntu/HallucinationVectorProject/utils.py:30: UserWarning: WARNING: Unsloth should be imported before trl, transformers, peft to ensure all optimizations are applied. Your code may run slower or encounter memory issues without these optimizations.\n",
            "\n",
            "Please restructure your imports with 'import unsloth' at the top of your file.\n",
            "  from unsloth import FastLanguageModel\n"
          ]
        },
        {
          "ename": "KeyError",
          "evalue": "'align_logprobs_with_mask'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[4], line 14\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# Import our project's config and utils modules\u001b[39;00m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mconfig\u001b[39;00m\n\u001b[0;32m---> 14\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mutils\u001b[39;00m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDefining combined logic for \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDynamic Alpha + Selective Steering\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m experiment...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m# --- 1. The SelectiveActivationSteerer Class ---\u001b[39;00m\n",
            "File \u001b[0;32m~/HallucinationVectorProject/utils.py:30\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m config\u001b[38;5;241m.\u001b[39mENVIRONMENT \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcolab\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m     28\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mgoogle\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcolab\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m userdata\n\u001b[0;32m---> 30\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01munsloth\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m FastLanguageModel\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mhuggingface_hub\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m HfApi, CommitOperationAdd\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mload_secrets\u001b[39m():\n",
            "File \u001b[0;32m~/HallucinationVectorProject/venv/lib/python3.10/site-packages/unsloth/__init__.py:211\u001b[0m\n\u001b[1;32m    208\u001b[0m     \u001b[38;5;66;03m# TODO: check triton for intel installed properly.\u001b[39;00m\n\u001b[1;32m    209\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[0;32m--> 211\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[1;32m    212\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m __version__\n\u001b[1;32m    213\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msave\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n",
            "File \u001b[0;32m~/HallucinationVectorProject/venv/lib/python3.10/site-packages/unsloth/models/__init__.py:15\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Copyright 2023-present Daniel Han-Chen & the Unsloth team. All rights reserved.\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Licensed under the Apache License, Version 2.0 (the \"License\");\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# See the License for the specific language governing permissions and\u001b[39;00m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# limitations under the License.\u001b[39;00m\n\u001b[0;32m---> 15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mllama\u001b[39;00m\u001b[38;5;250m     \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m FastLlamaModel\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mloader\u001b[39;00m\u001b[38;5;250m    \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m FastLanguageModel, FastVisionModel, FastTextModel, FastModel\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmistral\u001b[39;00m\u001b[38;5;250m   \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m FastMistralModel\n",
            "File \u001b[0;32m~/HallucinationVectorProject/venv/lib/python3.10/site-packages/unsloth/models/llama.py:3050\u001b[0m\n\u001b[1;32m   3047\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[1;32m   3048\u001b[0m \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[0;32m-> 3050\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mrl\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m PatchFastRL\n\u001b[1;32m   3051\u001b[0m PatchFastRL(FastLanguageModel \u001b[38;5;241m=\u001b[39m FastLlamaModel)\n",
            "File \u001b[0;32m~/HallucinationVectorProject/venv/lib/python3.10/site-packages/unsloth/models/rl.py:186\u001b[0m\n\u001b[1;32m    184\u001b[0m create_completion_attention_mask \u001b[38;5;241m=\u001b[39m RL_REPLACEMENTS[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcreate_completion_attention_mask\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    185\u001b[0m left_pack_padding                \u001b[38;5;241m=\u001b[39m RL_REPLACEMENTS[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mleft_pack_padding\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m--> 186\u001b[0m align_logprobs_with_mask         \u001b[38;5;241m=\u001b[39m \u001b[43mRL_REPLACEMENTS\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43malign_logprobs_with_mask\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[1;32m    188\u001b[0m RLTrainer_replacement \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'''\u001b[39m\n\u001b[1;32m    189\u001b[0m \u001b[38;5;124mimport os\u001b[39m\n\u001b[1;32m    190\u001b[0m \u001b[38;5;124mfrom typing import *\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    282\u001b[0m \u001b[38;5;124mpass\u001b[39m\n\u001b[1;32m    283\u001b[0m \u001b[38;5;124m'''\u001b[39m\n\u001b[1;32m    285\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_patch_trl_rl_trainers\u001b[39m(trainer_file \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgrpo_trainer\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    286\u001b[0m     \u001b[38;5;66;03m# Patch for vLLM and Unsloth PEFT\u001b[39;00m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'align_logprobs_with_mask'"
          ]
        }
      ],
      "source": [
        "import time\n",
        "import torch\n",
        "import sys\n",
        "from pathlib import Path\n",
        "from contextlib import contextmanager\n",
        "\n",
        "# Ensure project directory is in path before importing custom modules\n",
        "PROJECT_DIR = Path(\"/home/ubuntu/HallucinationVectorProject\")\n",
        "if str(PROJECT_DIR) not in sys.path:\n",
        "    sys.path.insert(0, str(PROJECT_DIR))\n",
        "\n",
        "# Import our project's config and utils modules\n",
        "import config\n",
        "import utils\n",
        "\n",
        "print(\"Defining combined logic for 'Dynamic Alpha + Selective Steering' experiment...\")\n",
        "\n",
        "# --- 1. The SelectiveActivationSteerer Class ---\n",
        "\n",
        "class SelectiveActivationSteerer:\n",
        "    def __init__(self, model, steering_vector, layer_idx, coeff=1.0, steering_token_limit=10):\n",
        "        self.model = model\n",
        "        self.vector = steering_vector\n",
        "        self.layer_idx = layer_idx\n",
        "        self.coeff = coeff\n",
        "        self.steering_token_limit = steering_token_limit\n",
        "        self._handle = None\n",
        "        self._layer_path = f\"model.layers.{self.layer_idx}\"\n",
        "        self.call_count = 0\n",
        "\n",
        "    def _hook_fn(self, module, ins, out):\n",
        "        self.call_count += 1\n",
        "        if self.call_count <= self.steering_token_limit:\n",
        "            steered_output = out[0] + (self.coeff * self.vector.to(out[0].device))\n",
        "            return (steered_output,) + out[1:]\n",
        "        return out\n",
        "\n",
        "    def __enter__(self):\n",
        "        self.call_count = 0\n",
        "        try:\n",
        "            layer = self.model.get_submodule(self._layer_path)\n",
        "            self._handle = layer.register_forward_hook(self._hook_fn)\n",
        "        except AttributeError:\n",
        "            raise AttributeError(f\"Could not find the layer at path: {self._layer_path}\")\n",
        "        return self\n",
        "\n",
        "    def __exit__(self, exc_type, exc_val, exc_tb):\n",
        "        if self._handle:\n",
        "            self._handle.remove()\n",
        "\n",
        "\n",
        "# --- 2. The New `answer_guarded_combined` Function ---\n",
        "# This function combines the logic from both successful ablations.\n",
        "\n",
        "def answer_guarded_combined(prompt_text: str, max_new_tokens: int = 128, steering_token_limit: int = 10):\n",
        "    \"\"\"\n",
        "    Generates a response using the guardrail with DYNAMIC alpha and SELECTIVE steering.\n",
        "    Enhanced for 70B model with proper device handling and memory management.\n",
        "    \"\"\"\n",
        "    start_time = time.time()\n",
        "\n",
        "    try:\n",
        "        risk_score = utils.get_hallucination_risk(\n",
        "            prompt_text, artifacts['model'], artifacts['tokenizer'],\n",
        "            artifacts['v_halluc'], artifacts['risk_classifier']\n",
        "        )\n",
        "\n",
        "        full_prompt = f\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\nYou are a helpful assistant.\\n<|eot_id|><|start_header_id|>user<|end_header_id|>\\nAnswer the following briefly.\\n{prompt_text}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n\"\n",
        "        inputs = artifacts['tokenizer'](full_prompt, return_tensors=\"pt\", max_length=4096, truncation=True).to(artifacts['model'].device)\n",
        "        input_token_length = inputs.input_ids.shape[1]\n",
        "\n",
        "        if risk_score < artifacts['thresholds']['tau_high']:\n",
        "            path = \"Fast Path (Untouched)\"\n",
        "            with torch.no_grad():\n",
        "                outputs = artifacts['model'].generate(\n",
        "                    **inputs, \n",
        "                    max_new_tokens=max_new_tokens, \n",
        "                    do_sample=False,\n",
        "                    pad_token_id=artifacts['tokenizer'].eos_token_id\n",
        "                )\n",
        "        else:\n",
        "            # From Dynamic Alpha Ablation: Calculate dynamic steering strength.\n",
        "            optimal_alpha = artifacts['thresholds']['optimal_alpha']\n",
        "            tau_high = artifacts['thresholds']['tau_high']\n",
        "            scaling_factor = (risk_score - tau_high) / (1.0 - tau_high + 1e-6) # Add epsilon for stability\n",
        "            dynamic_alpha = optimal_alpha * max(0, min(1, scaling_factor)) # Clamp between 0 and 1\n",
        "\n",
        "            path = f\"Combined Steer Path (α={dynamic_alpha:.2f}, N={steering_token_limit})\"\n",
        "\n",
        "            # From Selective N-Tokens Ablation: Use the steerer with a token limit.\n",
        "            # We pass our newly calculated `dynamic_alpha` as the coefficient.\n",
        "            with SelectiveActivationSteerer(\n",
        "                artifacts['model'], artifacts['v_halluc'], config.TARGET_LAYER,\n",
        "                coeff=dynamic_alpha,\n",
        "                steering_token_limit=steering_token_limit\n",
        "            ):\n",
        "                with torch.no_grad():\n",
        "                    outputs = artifacts['model'].generate(\n",
        "                        **inputs, \n",
        "                        max_new_tokens=max_new_tokens, \n",
        "                        do_sample=False,\n",
        "                        pad_token_id=artifacts['tokenizer'].eos_token_id\n",
        "                    )\n",
        "\n",
        "        answer = artifacts['tokenizer'].decode(outputs[0, input_token_length:], skip_special_tokens=True)\n",
        "        latency = time.time() - start_time\n",
        "\n",
        "        return {\"answer\": answer.strip(), \"risk_score\": risk_score, \"path_taken\": path, \"latency_seconds\": latency}\n",
        "    \n",
        "    except Exception as e:\n",
        "        print(f\"Error in answer_guarded_combined: {e}\")\n",
        "        latency = time.time() - start_time\n",
        "        return {\"answer\": \"\", \"risk_score\": 0.5, \"path_taken\": \"Error\", \"latency_seconds\": latency}\n",
        "\n",
        "print(\"✓ New function `answer_guarded_combined` is now defined and ready for the experiment.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Suppress Warnings**\n",
        "Suppresses specific sklearn warnings for cleaner output during evaluation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "OZPn3nOm5gLa"
      },
      "outputs": [],
      "source": [
        "import warnings\n",
        "\n",
        "warnings.filterwarnings(\n",
        "    \"ignore\",\n",
        "    message=\"X does not have valid feature names\",\n",
        "    category=UserWarning,\n",
        "    module=\"sklearn\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### **Run Combined Guardrail Evaluation**\n",
        "Runs the evaluation loop on the TruthfulQA test set, applying the combined guardrail and saving results for each prompt."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eHzIZVAI12fr",
        "outputId": "fb931eeb-a1b1-4332-8512-79a85cb82381"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "New guarded results will be saved to: /home/ubuntu/HallucinationVectorProject/results/llama-3.1-8b/combined_guarded_results.csv\n",
            "Loaded 617 test prompts from TruthfulQA\n",
            "Starting response generation for combined guardrail (Dynamic Alpha + Selective N=10)...\n",
            "Already processed: 173 prompts\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Combined Guardrail Evaluation:   0%|          | 0/617 [00:00<?, ?it/s]"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Combined Guardrail Evaluation:  28%|██▊       | 174/617 [00:01<00:04, 108.87it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Progress: 180/617 (29.2%) | Rate: 12.68 prompts/s | ETA: 0.6 min\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Combined Guardrail Evaluation:  30%|███       | 188/617 [00:31<02:24,  2.97it/s] "
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Progress: 190/617 (30.8%) | Rate: 5.07 prompts/s | ETA: 1.4 min\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Combined Guardrail Evaluation:  32%|███▏      | 200/617 [00:54<06:46,  1.03it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Progress: 200/617 (32.4%) | Rate: 3.70 prompts/s | ETA: 1.9 min\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Combined Guardrail Evaluation:  34%|███▍      | 210/617 [01:08<11:01,  1.63s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Progress: 210/617 (34.0%) | Rate: 3.09 prompts/s | ETA: 2.2 min\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Combined Guardrail Evaluation:  36%|███▌      | 220/617 [01:27<16:49,  2.54s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Progress: 220/617 (35.7%) | Rate: 2.51 prompts/s | ETA: 2.6 min\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Combined Guardrail Evaluation:  37%|███▋      | 230/617 [01:46<15:14,  2.36s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Progress: 230/617 (37.3%) | Rate: 2.16 prompts/s | ETA: 3.0 min\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Combined Guardrail Evaluation:  39%|███▉      | 240/617 [02:07<14:14,  2.27s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Progress: 240/617 (38.9%) | Rate: 1.89 prompts/s | ETA: 3.3 min\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Combined Guardrail Evaluation:  41%|████      | 250/617 [02:23<11:25,  1.87s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Progress: 250/617 (40.5%) | Rate: 1.74 prompts/s | ETA: 3.5 min\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Combined Guardrail Evaluation:  42%|████▏     | 260/617 [02:42<11:38,  1.96s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Progress: 260/617 (42.1%) | Rate: 1.60 prompts/s | ETA: 3.7 min\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Combined Guardrail Evaluation:  44%|████▍     | 270/617 [02:59<11:55,  2.06s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Progress: 270/617 (43.8%) | Rate: 1.50 prompts/s | ETA: 3.8 min\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Combined Guardrail Evaluation:  45%|████▌     | 280/617 [03:15<07:12,  1.28s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Progress: 280/617 (45.4%) | Rate: 1.43 prompts/s | ETA: 3.9 min\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Combined Guardrail Evaluation:  47%|████▋     | 290/617 [03:31<12:27,  2.28s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Progress: 290/617 (47.0%) | Rate: 1.37 prompts/s | ETA: 4.0 min\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Combined Guardrail Evaluation:  49%|████▊     | 300/617 [03:47<12:24,  2.35s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Progress: 300/617 (48.6%) | Rate: 1.32 prompts/s | ETA: 4.0 min\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Combined Guardrail Evaluation:  50%|█████     | 310/617 [04:03<10:10,  1.99s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Progress: 310/617 (50.2%) | Rate: 1.27 prompts/s | ETA: 4.0 min\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Combined Guardrail Evaluation:  52%|█████▏    | 320/617 [04:25<09:13,  1.87s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Progress: 320/617 (51.9%) | Rate: 1.21 prompts/s | ETA: 4.1 min\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Combined Guardrail Evaluation:  53%|█████▎    | 330/617 [04:46<10:28,  2.19s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Progress: 330/617 (53.5%) | Rate: 1.15 prompts/s | ETA: 4.2 min\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Combined Guardrail Evaluation:  55%|█████▌    | 340/617 [05:09<13:09,  2.85s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Progress: 340/617 (55.1%) | Rate: 1.10 prompts/s | ETA: 4.2 min\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Combined Guardrail Evaluation:  57%|█████▋    | 350/617 [05:28<10:58,  2.47s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Progress: 350/617 (56.7%) | Rate: 1.07 prompts/s | ETA: 4.2 min\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Combined Guardrail Evaluation:  58%|█████▊    | 360/617 [05:37<02:58,  1.44it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Progress: 360/617 (58.3%) | Rate: 1.07 prompts/s | ETA: 4.0 min\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Combined Guardrail Evaluation:  60%|█████▉    | 370/617 [05:53<04:47,  1.16s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Progress: 370/617 (60.0%) | Rate: 1.05 prompts/s | ETA: 3.9 min\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Combined Guardrail Evaluation:  62%|██████▏   | 380/617 [06:19<11:07,  2.82s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Progress: 380/617 (61.6%) | Rate: 1.00 prompts/s | ETA: 3.9 min\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Combined Guardrail Evaluation:  63%|██████▎   | 391/617 [06:33<05:18,  1.41s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Progress: 390/617 (63.2%) | Rate: 0.99 prompts/s | ETA: 3.8 min\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Combined Guardrail Evaluation:  65%|██████▍   | 400/617 [06:53<08:09,  2.26s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Progress: 400/617 (64.8%) | Rate: 0.97 prompts/s | ETA: 3.7 min\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Combined Guardrail Evaluation:  66%|██████▋   | 410/617 [07:11<06:54,  2.00s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Progress: 410/617 (66.5%) | Rate: 0.95 prompts/s | ETA: 3.6 min\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Combined Guardrail Evaluation:  68%|██████▊   | 420/617 [07:29<04:29,  1.37s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Progress: 420/617 (68.1%) | Rate: 0.94 prompts/s | ETA: 3.5 min\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Combined Guardrail Evaluation:  70%|██████▉   | 430/617 [07:46<03:46,  1.21s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Progress: 430/617 (69.7%) | Rate: 0.92 prompts/s | ETA: 3.4 min\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Combined Guardrail Evaluation:  71%|███████▏  | 440/617 [08:01<03:30,  1.19s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Progress: 440/617 (71.3%) | Rate: 0.91 prompts/s | ETA: 3.2 min\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Combined Guardrail Evaluation:  73%|███████▎  | 450/617 [08:26<07:15,  2.61s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Progress: 450/617 (72.9%) | Rate: 0.89 prompts/s | ETA: 3.1 min\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Combined Guardrail Evaluation:  75%|███████▍  | 460/617 [08:51<05:11,  1.98s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Progress: 460/617 (74.6%) | Rate: 0.87 prompts/s | ETA: 3.0 min\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Combined Guardrail Evaluation:  76%|███████▌  | 470/617 [09:16<07:13,  2.95s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Progress: 470/617 (76.2%) | Rate: 0.85 prompts/s | ETA: 2.9 min\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Combined Guardrail Evaluation:  78%|███████▊  | 480/617 [09:33<04:29,  1.97s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Progress: 480/617 (77.8%) | Rate: 0.84 prompts/s | ETA: 2.7 min\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Combined Guardrail Evaluation:  79%|███████▉  | 490/617 [09:51<04:20,  2.05s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Progress: 490/617 (79.4%) | Rate: 0.83 prompts/s | ETA: 2.6 min\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Combined Guardrail Evaluation:  81%|████████  | 500/617 [10:21<05:47,  2.97s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Progress: 500/617 (81.0%) | Rate: 0.80 prompts/s | ETA: 2.4 min\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Combined Guardrail Evaluation:  83%|████████▎ | 510/617 [10:43<04:15,  2.39s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Progress: 510/617 (82.7%) | Rate: 0.79 prompts/s | ETA: 2.3 min\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Combined Guardrail Evaluation:  84%|████████▍ | 520/617 [11:08<04:28,  2.77s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Progress: 520/617 (84.3%) | Rate: 0.78 prompts/s | ETA: 2.1 min\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Combined Guardrail Evaluation:  86%|████████▌ | 530/617 [11:23<02:25,  1.67s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Progress: 530/617 (85.9%) | Rate: 0.78 prompts/s | ETA: 1.9 min\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Combined Guardrail Evaluation:  88%|████████▊ | 540/617 [11:49<03:14,  2.52s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Progress: 540/617 (87.5%) | Rate: 0.76 prompts/s | ETA: 1.7 min\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Combined Guardrail Evaluation:  89%|████████▉ | 550/617 [12:04<01:24,  1.26s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Progress: 550/617 (89.1%) | Rate: 0.76 prompts/s | ETA: 1.5 min\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Combined Guardrail Evaluation:  91%|█████████ | 560/617 [12:23<01:27,  1.54s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Progress: 560/617 (90.8%) | Rate: 0.75 prompts/s | ETA: 1.3 min\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Combined Guardrail Evaluation:  92%|█████████▏| 570/617 [12:48<02:01,  2.58s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Progress: 570/617 (92.4%) | Rate: 0.74 prompts/s | ETA: 1.1 min\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Combined Guardrail Evaluation:  94%|█████████▍| 580/617 [13:14<01:24,  2.27s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Progress: 580/617 (94.0%) | Rate: 0.73 prompts/s | ETA: 0.8 min\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Combined Guardrail Evaluation:  96%|█████████▌| 590/617 [13:31<00:40,  1.50s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Progress: 590/617 (95.6%) | Rate: 0.73 prompts/s | ETA: 0.6 min\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Combined Guardrail Evaluation:  97%|█████████▋| 600/617 [13:49<00:25,  1.50s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Progress: 600/617 (97.2%) | Rate: 0.72 prompts/s | ETA: 0.4 min\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Combined Guardrail Evaluation:  99%|█████████▉| 610/617 [14:09<00:08,  1.18s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Progress: 610/617 (98.9%) | Rate: 0.72 prompts/s | ETA: 0.2 min\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Combined Guardrail Evaluation: 100%|██████████| 617/617 [14:23<00:00,  1.40s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "✓ Combined guardrail evaluation complete in 14.39 minutes\n",
            "Results saved to: /home/ubuntu/HallucinationVectorProject/results/llama-3.1-8b/combined_guarded_results.csv\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# --- EXPERIMENT PARAMETER ---\n",
        "STEERING_TOKEN_LIMIT = 10 # The 'N' for our selective steering\n",
        "\n",
        "# Use local paths\n",
        "GUARDED_RESULTS_PATH_COMBINED = RESULTS_DIR / \"combined_guarded_results.csv\"\n",
        "BASELINE_RESULTS_PATH = RESULTS_DIR / \"ablation_2_baseline_results_truthfulqa.csv\"\n",
        "\n",
        "print(f\"New guarded results will be saved to: {GUARDED_RESULTS_PATH_COMBINED}\")\n",
        "\n",
        "# Memory management helper\n",
        "def check_and_clear_memory(threshold_gb=60):\n",
        "    \"\"\"Clear GPU cache if memory usage exceeds threshold.\"\"\"\n",
        "    if torch.cuda.is_available():\n",
        "        for i in range(torch.cuda.device_count()):\n",
        "            allocated = torch.cuda.memory_allocated(i) / 1024**3\n",
        "            if allocated > threshold_gb:\n",
        "                print(f\"GPU {i} memory ({allocated:.2f}GB) exceeds threshold. Clearing cache...\")\n",
        "                torch.cuda.empty_cache()\n",
        "                return True\n",
        "    return False\n",
        "\n",
        "# Load the test set (using local path)\n",
        "test_df = pd.read_csv(DATA_DIR / \"final_test_set_truthfulqa.csv\")\n",
        "print(f\"Loaded {len(test_df)} test prompts from TruthfulQA\")\n",
        "\n",
        "# --- Resilient Evaluation Loop ---\n",
        "guarded_headers = ['prompt', 'answer', 'risk_score', 'path_taken', 'latency_seconds']\n",
        "utils.initialize_csv(GUARDED_RESULTS_PATH_COMBINED, guarded_headers)\n",
        "\n",
        "processed_guarded = utils.load_processed_prompts(GUARDED_RESULTS_PATH_COMBINED)\n",
        "\n",
        "print(f\"Starting response generation for combined guardrail (Dynamic Alpha + Selective N={STEERING_TOKEN_LIMIT})...\")\n",
        "print(f\"Already processed: {len(processed_guarded)} prompts\")\n",
        "\n",
        "start_time = time.time()\n",
        "processed_count = len(processed_guarded)\n",
        "\n",
        "for idx, row in tqdm(test_df.iterrows(), total=len(test_df), desc=\"Combined Guardrail Evaluation\"):\n",
        "    prompt = row['Question']\n",
        "\n",
        "    # Guarded Run\n",
        "    if prompt not in processed_guarded:\n",
        "        try:\n",
        "            result = answer_guarded_combined(prompt, steering_token_limit=STEERING_TOKEN_LIMIT)\n",
        "            with open(GUARDED_RESULTS_PATH_COMBINED, 'a', newline='', encoding='utf-8') as f:\n",
        "                csv.writer(f).writerow([prompt] + list(result.values()))\n",
        "            \n",
        "            processed_count += 1\n",
        "            \n",
        "            # Progress tracking and memory management\n",
        "            if processed_count % 10 == 0:\n",
        "                elapsed = time.time() - start_time\n",
        "                rate = processed_count / elapsed if elapsed > 0 else 0\n",
        "                remaining = len(test_df) - processed_count\n",
        "                eta = remaining / rate if rate > 0 else 0\n",
        "                print(f\"Progress: {processed_count}/{len(test_df)} ({processed_count/len(test_df)*100:.1f}%) | \"\n",
        "                      f\"Rate: {rate:.2f} prompts/s | ETA: {eta/60:.1f} min\")\n",
        "                check_and_clear_memory()\n",
        "                \n",
        "        except Exception as e:\n",
        "            print(f\"Error on guarded prompt: {prompt[:50]}... Error: {e}\")\n",
        "\n",
        "print(f\"\\n✓ Combined guardrail evaluation complete in {(time.time() - start_time)/60:.2f} minutes\")\n",
        "print(f\"Results saved to: {GUARDED_RESULTS_PATH_COMBINED}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Define the Baseline Generation Function\n",
        "This function will generate an answer from the original, unguarded model. It includes the requested prompt prefix and decodes only the new tokens."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Baseline generation function `generate_baseline` is defined.\n"
          ]
        }
      ],
      "source": [
        "import time\n",
        "\n",
        "def generate_baseline(prompt_text: str, max_new_tokens: int = 128):\n",
        "    \"\"\"\n",
        "    Generates a response from the unguarded baseline model.\n",
        "    \"\"\"\n",
        "    start_time = time.time()\n",
        "\n",
        "    # Prepend the required instruction to the prompt\n",
        "    full_prompt = f\"<|begin_of_text|><|start_header_id|>system<|end_header_id|> You are a helpful AI assistant. <|eot_id|><|start_header_id|>user<|end_header_id|> Answer the following question briefly:\\n{prompt_text} <|eot_id|><|start_header_id|>assistant<|end_header_id|>\"\n",
        "\n",
        "    inputs = artifacts['tokenizer'](full_prompt, return_tensors=\"pt\").to(artifacts['model'].device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = artifacts['model'].generate(\n",
        "            **inputs,\n",
        "            max_new_tokens=max_new_tokens,\n",
        "            do_sample=False,\n",
        "            pad_token_id=artifacts['tokenizer'].eos_token_id\n",
        "        )\n",
        "\n",
        "    # Decode only the newly generated tokens, skipping the prompt\n",
        "    input_token_length = inputs.input_ids.shape[1]\n",
        "    newly_generated_tokens = outputs[0, input_token_length:]\n",
        "    answer = artifacts['tokenizer'].decode(newly_generated_tokens, skip_special_tokens=True)\n",
        "\n",
        "    end_time = time.time()\n",
        "    latency = end_time - start_time\n",
        "\n",
        "    return {\n",
        "        \"answer\": answer,\n",
        "        \"latency_seconds\": latency\n",
        "    }\n",
        "\n",
        "print(\"Baseline generation function `generate_baseline` is defined.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Baseline results will be saved to: /home/ubuntu/HallucinationVectorProject/baseline_results_truthfulqa.csv\n",
            "Found 0 already processed prompts for the Baseline run.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Evaluating Prompts: 100%|██████████| 617/617 [22:59<00:00,  2.24s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--- Evaluation Runs Complete ---\n",
            "Total baseline results saved: 617\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "BASELINE_RESULTS_PATH = os.path.join(PROJECT_DIR, \"baseline_results_truthfulqa.csv\")\n",
        "\n",
        "print(f\"Baseline results will be saved to: {BASELINE_RESULTS_PATH}\")\n",
        "\n",
        "# --- Helper function to initialize CSV files with headers ---\n",
        "def initialize_csv(file_path, headers):\n",
        "    if not os.path.exists(file_path):\n",
        "        with open(file_path, 'w', newline='', encoding='utf-8') as f:\n",
        "            writer = csv.writer(f)\n",
        "            writer.writerow(headers)\n",
        "        return set()\n",
        "    else:\n",
        "        # Load existing prompts to know where to resume from\n",
        "        df_existing = pd.read_csv(file_path)\n",
        "        return set(df_existing['prompt'].tolist())\n",
        "\n",
        "# --- Initialize CSVs and get the set of already processed prompts ---\n",
        "baseline_headers = ['prompt', 'answer', 'latency_seconds']\n",
        "\n",
        "processed_baseline = initialize_csv(BASELINE_RESULTS_PATH, baseline_headers)\n",
        "\n",
        "print(f\"Found {len(processed_baseline)} already processed prompts for the Baseline run.\")\n",
        "\n",
        "# --- Main Evaluation Loop ---\n",
        "if test_df is not None:\n",
        "    # Use tqdm for a progress bar, which will handle progress indication\n",
        "    for index, row in tqdm(test_df.iterrows(), total=len(test_df), desc=\"Evaluating Prompts\"):\n",
        "        prompt = row['Question']\n",
        "\n",
        "        # --- Baseline Run ---\n",
        "        if prompt not in processed_baseline:\n",
        "            try:\n",
        "                baseline_result = generate_baseline(prompt)\n",
        "\n",
        "                # Append result immediately to the CSV\n",
        "                with open(BASELINE_RESULTS_PATH, 'a', newline='', encoding='utf-8') as f:\n",
        "                    writer = csv.writer(f)\n",
        "                    writer.writerow([\n",
        "                        prompt,\n",
        "                        baseline_result.get('answer'),\n",
        "                        baseline_result.get('latency_seconds')\n",
        "                    ])\n",
        "                processed_baseline.add(prompt)\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"  -> ERROR during baseline run for prompt: '{prompt}'. Error: {e}\")\n",
        "                with open(BASELINE_RESULTS_PATH, 'a', newline='', encoding='utf-8') as f:\n",
        "                    writer = csv.writer(f)\n",
        "                    writer.writerow([prompt, f\"ERROR: {e}\", -1.0])\n",
        "                processed_baseline.add(prompt)\n",
        "\n",
        "    print(\"\\n--- Evaluation Runs Complete ---\")\n",
        "    # Final check\n",
        "    final_baseline_df = pd.read_csv(BASELINE_RESULTS_PATH)\n",
        "    print(f\"Total baseline results saved: {len(final_baseline_df)}\")\n",
        "else:\n",
        "    print(\"Skipping evaluation loop as test_df was not loaded.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### **Run Judging, Analyze, and Summarize Results**\n",
        "Runs the judging process on generated answers, merges with ground truth, and computes final performance metrics for the combined guardrail experiment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 367
        },
        "id": "dBHYCOnl3ED6",
        "outputId": "f8b9262a-8e71-49b1-f02f-0a13aa0e7bca"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading datasets for judging and analysis...\n",
            "Guarded results: 617 prompts\n",
            "Baseline results: 617 prompts\n",
            "Loading secrets...\n",
            "Secrets loaded successfully.\n",
            "\n",
            "Starting judging process for combined guardrail results...\n",
            "\n",
            "--- Starting Corrected Judging Process for combined_guarded_judged_results.csv ---\n",
            "Found 454 already judged prompts. Resuming...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Judging combined_guarded_judged_results.csv:   0%|          | 0/617 [00:00<?, ?it/s]"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Judging combined_guarded_judged_results.csv: 100%|██████████| 617/617 [16:52<00:00,  1.64s/it]\n",
            "Judging combined_guarded_judged_results.csv: 100%|██████████| 617/617 [16:52<00:00,  1.64s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✓ Judging complete in 16.87 minutes\n",
            "\n",
            "--- Starting Corrected Judging Process for baseline_judged_results_truthfulqa.csv ---\n",
            "Initialized CSV file at: /home/ubuntu/HallucinationVectorProject/results/llama-3.1-8b/baseline_judged_results_truthfulqa.csv\n",
            "Found 0 already judged prompts. Resuming...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Judging baseline_judged_results_truthfulqa.csv: 100%|██████████| 617/617 [1:02:31<00:00,  6.08s/it]\n",
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Analyzing final performance metrics...\n"
          ]
        },
        {
          "ename": "ParserError",
          "evalue": "Error tokenizing data. C error: Expected 14 fields in line 455, saw 27\n",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mParserError\u001b[0m                               Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[11], line 61\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;66;03m# --- Analyze and Print Final Report ---\u001b[39;00m\n\u001b[1;32m     59\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mAnalyzing final performance metrics...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 61\u001b[0m guarded_judged_df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mGUARDED_JUDGED_PATH_COMBINED\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     62\u001b[0m baseline_judged_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(BASELINE_JUDGED_RESULTS_PATH)\n\u001b[1;32m     64\u001b[0m baseline_accuracy \u001b[38;5;241m=\u001b[39m baseline_judged_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mis_correct\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mmean()\n",
            "File \u001b[0;32m~/HallucinationVectorProject/venv/lib/python3.10/site-packages/pandas/io/parsers/readers.py:1026\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1013\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m   1014\u001b[0m     dialect,\n\u001b[1;32m   1015\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1022\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[1;32m   1023\u001b[0m )\n\u001b[1;32m   1024\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m-> 1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/HallucinationVectorProject/venv/lib/python3.10/site-packages/pandas/io/parsers/readers.py:626\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n\u001b[1;32m    625\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m parser:\n\u001b[0;32m--> 626\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mparser\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnrows\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/HallucinationVectorProject/venv/lib/python3.10/site-packages/pandas/io/parsers/readers.py:1923\u001b[0m, in \u001b[0;36mTextFileReader.read\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m   1916\u001b[0m nrows \u001b[38;5;241m=\u001b[39m validate_integer(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnrows\u001b[39m\u001b[38;5;124m\"\u001b[39m, nrows)\n\u001b[1;32m   1917\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1918\u001b[0m     \u001b[38;5;66;03m# error: \"ParserBase\" has no attribute \"read\"\u001b[39;00m\n\u001b[1;32m   1919\u001b[0m     (\n\u001b[1;32m   1920\u001b[0m         index,\n\u001b[1;32m   1921\u001b[0m         columns,\n\u001b[1;32m   1922\u001b[0m         col_dict,\n\u001b[0;32m-> 1923\u001b[0m     ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[attr-defined]\u001b[39;49;00m\n\u001b[1;32m   1924\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnrows\u001b[49m\n\u001b[1;32m   1925\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1926\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[1;32m   1927\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclose()\n",
            "File \u001b[0;32m~/HallucinationVectorProject/venv/lib/python3.10/site-packages/pandas/io/parsers/c_parser_wrapper.py:234\u001b[0m, in \u001b[0;36mCParserWrapper.read\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m    232\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    233\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlow_memory:\n\u001b[0;32m--> 234\u001b[0m         chunks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_reader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_low_memory\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnrows\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    235\u001b[0m         \u001b[38;5;66;03m# destructive to chunks\u001b[39;00m\n\u001b[1;32m    236\u001b[0m         data \u001b[38;5;241m=\u001b[39m _concatenate_chunks(chunks)\n",
            "File \u001b[0;32mpandas/_libs/parsers.pyx:838\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader.read_low_memory\u001b[0;34m()\u001b[0m\n",
            "File \u001b[0;32mpandas/_libs/parsers.pyx:905\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._read_rows\u001b[0;34m()\u001b[0m\n",
            "File \u001b[0;32mpandas/_libs/parsers.pyx:874\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._tokenize_rows\u001b[0;34m()\u001b[0m\n",
            "File \u001b[0;32mpandas/_libs/parsers.pyx:891\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._check_tokenize_status\u001b[0;34m()\u001b[0m\n",
            "File \u001b[0;32mpandas/_libs/parsers.pyx:2061\u001b[0m, in \u001b[0;36mpandas._libs.parsers.raise_parser_error\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mParserError\u001b[0m: Error tokenizing data. C error: Expected 14 fields in line 455, saw 27\n"
          ]
        }
      ],
      "source": [
        "# WORKAROUND: Import only what we need for judging (no model loading)\n",
        "import sys\n",
        "from pathlib import Path\n",
        "\n",
        "PROJECT_DIR = Path(\"/home/ubuntu/HallucinationVectorProject\")\n",
        "if str(PROJECT_DIR) not in sys.path:\n",
        "    sys.path.insert(0, str(PROJECT_DIR))\n",
        "\n",
        "# Now import the judging function\n",
        "from evaluate_guardrail import run_judging_process\n",
        "import pandas as pd\n",
        "import time\n",
        "import os\n",
        "\n",
        "# Import utils and config (FastLanguageModel import is now commented out)\n",
        "import utils\n",
        "import config\n",
        "\n",
        "# --- Define paths for the analysis (using local paths) ---\n",
        "DATA_DIR = PROJECT_DIR / \"data\"\n",
        "RESULTS_DIR = PROJECT_DIR / \"results\" / \"llama-3.1-8b\"\n",
        "GUARDED_JUDGED_PATH_COMBINED = RESULTS_DIR / \"combined_guarded_judged_results.csv\"\n",
        "BASELINE_JUDGED_RESULTS_PATH = RESULTS_DIR / \"baseline_judged_results_truthfulqa.csv\"\n",
        "GUARDED_RESULTS_PATH_COMBINED = RESULTS_DIR / \"combined_guarded_results.csv\"\n",
        "BASELINE_RESULTS_PATH = os.path.join(PROJECT_DIR, \"baseline_results_truthfulqa.csv\")\n",
        "\n",
        "print(\"Loading datasets for judging and analysis...\")\n",
        "\n",
        "# Load the test set\n",
        "test_df = pd.read_csv(DATA_DIR / \"final_test_set_truthfulqa.csv\")\n",
        "\n",
        "# Load the newly generated results\n",
        "guarded_df = pd.read_csv(GUARDED_RESULTS_PATH_COMBINED)\n",
        "baseline_df = pd.read_csv(BASELINE_RESULTS_PATH)\n",
        "\n",
        "# Merge with ground truth\n",
        "guarded_merged_df = pd.merge(guarded_df, test_df, left_on='prompt', right_on='Question', how='left')\n",
        "baseline_merged_df = pd.merge(baseline_df, test_df, left_on='prompt', right_on='Question', how='left')\n",
        "\n",
        "print(f\"Guarded results: {len(guarded_merged_df)} prompts\")\n",
        "print(f\"Baseline results: {len(baseline_merged_df)} prompts\")\n",
        "\n",
        "# --- Run Judging with retry logic for network stability ---\n",
        "secrets = utils.load_secrets()\n",
        "\n",
        "print(\"\\nStarting judging process for combined guardrail results...\")\n",
        "start_time = time.time()\n",
        "\n",
        "try:\n",
        "    run_judging_process(guarded_merged_df, GUARDED_JUDGED_PATH_COMBINED, secrets['SCALEDOWN_API_KEY'])\n",
        "    print(f\"✓ Judging complete in {(time.time() - start_time)/60:.2f} minutes\")\n",
        "except Exception as e:\n",
        "    print(f\"Error during judging: {e}\")\n",
        "\n",
        "# Assuming baseline is already judged, if not, uncomment below\n",
        "run_judging_process(baseline_merged_df, BASELINE_JUDGED_RESULTS_PATH, secrets['SCALEDOWN_API_KEY'])\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Analyzing final performance metrics...\n"
          ]
        },
        {
          "ename": "ParserError",
          "evalue": "Error tokenizing data. C error: Expected 14 fields in line 455, saw 27\n",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mParserError\u001b[0m                               Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[12], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# --- Analyze and Print Final Report ---\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mAnalyzing final performance metrics...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 4\u001b[0m guarded_judged_df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mGUARDED_JUDGED_PATH_COMBINED\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m baseline_judged_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(BASELINE_JUDGED_RESULTS_PATH)\n\u001b[1;32m      7\u001b[0m baseline_accuracy \u001b[38;5;241m=\u001b[39m baseline_judged_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mis_correct\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mmean()\n",
            "File \u001b[0;32m~/HallucinationVectorProject/venv/lib/python3.10/site-packages/pandas/io/parsers/readers.py:1026\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1013\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m   1014\u001b[0m     dialect,\n\u001b[1;32m   1015\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1022\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[1;32m   1023\u001b[0m )\n\u001b[1;32m   1024\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m-> 1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/HallucinationVectorProject/venv/lib/python3.10/site-packages/pandas/io/parsers/readers.py:626\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n\u001b[1;32m    625\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m parser:\n\u001b[0;32m--> 626\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mparser\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnrows\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/HallucinationVectorProject/venv/lib/python3.10/site-packages/pandas/io/parsers/readers.py:1923\u001b[0m, in \u001b[0;36mTextFileReader.read\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m   1916\u001b[0m nrows \u001b[38;5;241m=\u001b[39m validate_integer(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnrows\u001b[39m\u001b[38;5;124m\"\u001b[39m, nrows)\n\u001b[1;32m   1917\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1918\u001b[0m     \u001b[38;5;66;03m# error: \"ParserBase\" has no attribute \"read\"\u001b[39;00m\n\u001b[1;32m   1919\u001b[0m     (\n\u001b[1;32m   1920\u001b[0m         index,\n\u001b[1;32m   1921\u001b[0m         columns,\n\u001b[1;32m   1922\u001b[0m         col_dict,\n\u001b[0;32m-> 1923\u001b[0m     ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[attr-defined]\u001b[39;49;00m\n\u001b[1;32m   1924\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnrows\u001b[49m\n\u001b[1;32m   1925\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1926\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[1;32m   1927\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclose()\n",
            "File \u001b[0;32m~/HallucinationVectorProject/venv/lib/python3.10/site-packages/pandas/io/parsers/c_parser_wrapper.py:234\u001b[0m, in \u001b[0;36mCParserWrapper.read\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m    232\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    233\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlow_memory:\n\u001b[0;32m--> 234\u001b[0m         chunks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_reader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_low_memory\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnrows\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    235\u001b[0m         \u001b[38;5;66;03m# destructive to chunks\u001b[39;00m\n\u001b[1;32m    236\u001b[0m         data \u001b[38;5;241m=\u001b[39m _concatenate_chunks(chunks)\n",
            "File \u001b[0;32mpandas/_libs/parsers.pyx:838\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader.read_low_memory\u001b[0;34m()\u001b[0m\n",
            "File \u001b[0;32mpandas/_libs/parsers.pyx:905\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._read_rows\u001b[0;34m()\u001b[0m\n",
            "File \u001b[0;32mpandas/_libs/parsers.pyx:874\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._tokenize_rows\u001b[0;34m()\u001b[0m\n",
            "File \u001b[0;32mpandas/_libs/parsers.pyx:891\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._check_tokenize_status\u001b[0;34m()\u001b[0m\n",
            "File \u001b[0;32mpandas/_libs/parsers.pyx:2061\u001b[0m, in \u001b[0;36mpandas._libs.parsers.raise_parser_error\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mParserError\u001b[0m: Error tokenizing data. C error: Expected 14 fields in line 455, saw 27\n"
          ]
        }
      ],
      "source": [
        "# --- Analyze and Print Final Report ---\n",
        "print(\"\\nAnalyzing final performance metrics...\")\n",
        "\n",
        "# Read CSVs with error handling for malformed rows\n",
        "try:\n",
        "    guarded_judged_df = pd.read_csv(GUARDED_JUDGED_PATH_COMBINED, on_bad_lines='skip', engine='python')\n",
        "    print(f\"Loaded guarded results: {len(guarded_judged_df)} rows\")\n",
        "except Exception as e:\n",
        "    print(f\"Error reading guarded results with python engine: {e}\")\n",
        "    # Fallback: try with quoting\n",
        "    guarded_judged_df = pd.read_csv(GUARDED_JUDGED_PATH_COMBINED, quoting=1, on_bad_lines='skip')\n",
        "    print(f\"Loaded guarded results (fallback): {len(guarded_judged_df)} rows\")\n",
        "\n",
        "try:\n",
        "    baseline_judged_df = pd.read_csv(BASELINE_JUDGED_RESULTS_PATH, on_bad_lines='skip', engine='python')\n",
        "    print(f\"Loaded baseline results: {len(baseline_judged_df)} rows\")\n",
        "except Exception as e:\n",
        "    print(f\"Error reading baseline results with python engine: {e}\")\n",
        "    baseline_judged_df = pd.read_csv(BASELINE_JUDGED_RESULTS_PATH, quoting=1, on_bad_lines='skip')\n",
        "    print(f\"Loaded baseline results (fallback): {len(baseline_judged_df)} rows\")\n",
        "\n",
        "baseline_accuracy = baseline_judged_df['is_correct'].mean()\n",
        "guarded_accuracy = guarded_judged_df['is_correct'].mean()\n",
        "baseline_error_rate = 1 - baseline_accuracy\n",
        "guarded_error_rate = 1 - guarded_accuracy\n",
        "relative_error_reduction = (baseline_error_rate - guarded_error_rate) / baseline_error_rate if baseline_error_rate > 0 else 0\n",
        "baseline_latency = baseline_judged_df['latency_seconds'].mean()\n",
        "guarded_latency = guarded_judged_df['latency_seconds'].mean()\n",
        "latency_increase_percent = (guarded_latency - baseline_latency) / baseline_latency * 100\n",
        "\n",
        "summary_data = {\n",
        "    \"Metric\": [\"Accuracy\", \"Hallucination Rate\", \"Avg Latency (s)\", \"Relative Error Reduction\", \"Latency Increase\"],\n",
        "    \"Baseline Model\": [f\"{baseline_accuracy:.2%}\", f\"{baseline_error_rate:.2%}\", f\"{baseline_latency:.2f}\", \"N/A\", \"N/A\"],\n",
        "    \"Guarded Model (Combined)\": [f\"{guarded_accuracy:.2%}\", f\"{guarded_error_rate:.2%}\", f\"{guarded_latency:.2f}\", f\"{relative_error_reduction:.2%}\", f\"{latency_increase_percent:+.2f}%\"],\n",
        "}\n",
        "summary_df = pd.DataFrame(summary_data)\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"FINAL PERFORMANCE SUMMARY (Combined Dynamic Alpha + Selective N-Tokens)\")\n",
        "print(\"=\"*80)\n",
        "print(summary_df.to_string(index=False))\n",
        "print(\"=\"*80)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "0a130b649922465fbaaa39360bc182b3": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0ff04e29983d44f4b0500bd187413494",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_fb5f686ae0d846daa92a3f7eca0cb09c",
            "value": 1
          }
        },
        "0af727690a8c45189de6b891f5544520": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0ed0d6557757456b9e17a68f962a6101": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0ff04e29983d44f4b0500bd187413494": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "12bd09bfc4d64f2897444365ae8fcf55": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "16c66f714ed4455d98d2d908aac474e9": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0ed0d6557757456b9e17a68f962a6101",
            "placeholder": "​",
            "style": "IPY_MODEL_7085b78429354064a9fab9ce02859ab1",
            "value": " 345/345 [00:00&lt;00:00, 24.5kB/s]"
          }
        },
        "1ad470b0a62d4601a1519bae23982985": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1cc59559f4714bb68164d44fbe9c4388": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_58e1ff15b9af4bab8225b462de199ef1",
              "IPY_MODEL_d9ebdf759a5347609c1ae2f30d7a73a1",
              "IPY_MODEL_16c66f714ed4455d98d2d908aac474e9"
            ],
            "layout": "IPY_MODEL_659934e23afd4d2ba94f672058a226c8"
          }
        },
        "1ce0fe58bdf04419bba04b24bb6b3d44": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_390a34c1d7c44413888da54078fed4ee",
            "placeholder": "​",
            "style": "IPY_MODEL_f757bbf5954e4ad093e50081ce4d4187",
            "value": " 51.1k/? [00:00&lt;00:00, 5.25MB/s]"
          }
        },
        "21b5555a566947a395837924a0425748": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "27cd1ec6893242ceade77c811d0f4c20": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2a8f6933fd824103be3ae29cfb4aff07": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "2b09d3f2094d4906a56883d3f467bcba": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_cc268fe08c5649c6a172c887ea637b8e",
            "placeholder": "​",
            "style": "IPY_MODEL_baffd1a874b84e69bf0dabefffa6fa5c",
            "value": " 220/220 [00:00&lt;00:00, 22.8kB/s]"
          }
        },
        "33b418ea3b974d788e1b0e736ae59414": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "390a34c1d7c44413888da54078fed4ee": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "39c05fc5cece43c696cb21be0d7598fb": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0af727690a8c45189de6b891f5544520",
            "placeholder": "​",
            "style": "IPY_MODEL_9ef056edfefb42859c1017479e729f8a",
            "value": "tokenizer_config.json: "
          }
        },
        "4f5b35cbc6774df3b1fe397b14640933": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5848feacc80a4506acd04c7ab328720c": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "58e1ff15b9af4bab8225b462de199ef1": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_92f6b3ced073463089b038f2bc506e87",
            "placeholder": "​",
            "style": "IPY_MODEL_f104dba960124583a9e01cdedb77fec2",
            "value": "special_tokens_map.json: 100%"
          }
        },
        "5937ff2ff09145a1aed178187eec6b11": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5848feacc80a4506acd04c7ab328720c",
            "placeholder": "​",
            "style": "IPY_MODEL_6af94307d3504f029600735742ec2ba8",
            "value": " 5.70G/5.70G [00:40&lt;00:00, 197MB/s]"
          }
        },
        "59ec59c5eb22439989621785442232a3": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "5a8876aa9160455aaace1cff34d3053d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_853eae85943e4f059bb4f284ab03857a",
            "placeholder": "​",
            "style": "IPY_MODEL_27cd1ec6893242ceade77c811d0f4c20",
            "value": " 9.09M/? [00:00&lt;00:00, 88.8MB/s]"
          }
        },
        "659934e23afd4d2ba94f672058a226c8": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6af94307d3504f029600735742ec2ba8": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7085b78429354064a9fab9ce02859ab1": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "79cbba6bd54549bd8131cd887d2e0708": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "79ee4dd2310a4113b30d00dd7b9f42e9": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "853eae85943e4f059bb4f284ab03857a": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "85efeb56ef7b4ec2b69753e2d8d16abb": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "8b98fbbe2d51452aa3605cc56fd45703": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_33b418ea3b974d788e1b0e736ae59414",
            "max": 5702746403,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_85efeb56ef7b4ec2b69753e2d8d16abb",
            "value": 5702746403
          }
        },
        "8bfec65739ab4b2f8c11a995172f3623": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "905c490e14704d99a8050dcdfea8a4aa": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "92f6b3ced073463089b038f2bc506e87": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "93dc8d4505ff405585edcf873831fccb": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_21b5555a566947a395837924a0425748",
            "placeholder": "​",
            "style": "IPY_MODEL_ce416e14219d473aa127531c4e806b0f",
            "value": "tokenizer.json: "
          }
        },
        "9ef056edfefb42859c1017479e729f8a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a2929e498fd2482681c9ab3dd7a18856": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a4a1ed5428f14053b589bd5760905c23",
            "max": 220,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_eb9d8be9ae874961b365535a32d3bdfa",
            "value": 220
          }
        },
        "a4a1ed5428f14053b589bd5760905c23": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b0f86b759fa34618840fd7f6426ca31b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_b2c0965a7c8f4e06841bc97baf6913cc",
              "IPY_MODEL_8b98fbbe2d51452aa3605cc56fd45703",
              "IPY_MODEL_5937ff2ff09145a1aed178187eec6b11"
            ],
            "layout": "IPY_MODEL_1ad470b0a62d4601a1519bae23982985"
          }
        },
        "b1caba83cb704bd98a96128eece691e0": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_93dc8d4505ff405585edcf873831fccb",
              "IPY_MODEL_bb2b2b8149da46c2909a9de140c1be37",
              "IPY_MODEL_5a8876aa9160455aaace1cff34d3053d"
            ],
            "layout": "IPY_MODEL_e2f0f384cb344d49af8c1eb063f0c370"
          }
        },
        "b2bf26fd9bef4f3da213f856ee74260c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_de91c5159db443b0bf0caf88e72d312b",
              "IPY_MODEL_a2929e498fd2482681c9ab3dd7a18856",
              "IPY_MODEL_2b09d3f2094d4906a56883d3f467bcba"
            ],
            "layout": "IPY_MODEL_79cbba6bd54549bd8131cd887d2e0708"
          }
        },
        "b2c0965a7c8f4e06841bc97baf6913cc": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8bfec65739ab4b2f8c11a995172f3623",
            "placeholder": "​",
            "style": "IPY_MODEL_4f5b35cbc6774df3b1fe397b14640933",
            "value": "model.safetensors: 100%"
          }
        },
        "baffd1a874b84e69bf0dabefffa6fa5c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "bb2b2b8149da46c2909a9de140c1be37": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_79ee4dd2310a4113b30d00dd7b9f42e9",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_2a8f6933fd824103be3ae29cfb4aff07",
            "value": 1
          }
        },
        "cc268fe08c5649c6a172c887ea637b8e": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ccec744d07344082beaad09c8e756b59": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ce416e14219d473aa127531c4e806b0f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d9ebdf759a5347609c1ae2f30d7a73a1": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ee561b9b6117400b80def264d5cc0c5a",
            "max": 345,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_59ec59c5eb22439989621785442232a3",
            "value": 345
          }
        },
        "de91c5159db443b0bf0caf88e72d312b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_12bd09bfc4d64f2897444365ae8fcf55",
            "placeholder": "​",
            "style": "IPY_MODEL_905c490e14704d99a8050dcdfea8a4aa",
            "value": "generation_config.json: 100%"
          }
        },
        "e2f0f384cb344d49af8c1eb063f0c370": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "eb9d8be9ae874961b365535a32d3bdfa": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "ee561b9b6117400b80def264d5cc0c5a": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f104dba960124583a9e01cdedb77fec2": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f757bbf5954e4ad093e50081ce4d4187": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "fb5f686ae0d846daa92a3f7eca0cb09c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "fb9418c321b74ca2905264a7015a3cf7": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_39c05fc5cece43c696cb21be0d7598fb",
              "IPY_MODEL_0a130b649922465fbaaa39360bc182b3",
              "IPY_MODEL_1ce0fe58bdf04419bba04b24bb6b3d44"
            ],
            "layout": "IPY_MODEL_ccec744d07344082beaad09c8e756b59"
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
